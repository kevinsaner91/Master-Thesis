% $ biblatex auxiliary file $
% $ biblatex bbl format version 3.1 $
% Do not modify the above lines!
%
% This is an auxiliary file used by the 'biblatex' package.
% This file may safely be deleted. It will be recreated as
% required.
%
\begingroup
\makeatletter
\@ifundefined{ver@biblatex.sty}
  {\@latex@error
     {Missing 'biblatex' package}
     {The bibliography requires the 'biblatex' package.}
      \aftergroup\endinput}
  {}
\endgroup

\datalist[entry]{nyt/global//global/global}
  \entry{Aggarwal2013}{book}{}
    \name{author}{1}{}{%
      {{hash=ACC}{%
         family={Aggarwal},
         familyi={A\bibinitperiod},
         given={Charu\bibnamedelima C.},
         giveni={C\bibinitperiod\bibinitdelim C\bibinitperiod},
      }}%
    }
    \strng{namehash}{ACC1}
    \strng{fullhash}{ACC1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{labelyear}{2013}
    \field{labeldatesource}{}
    \field{sortinit}{A}
    \field{sortinithash}{A}
    \field{abstract}{%
    With the increasing advances in hardware technology for data collection,
  and advances in software technology (databases) for data organization,
  computer scientists have increasingly participated in the latest advancements
  of the outlier analysis field. Computer scientists, specifically, approach
  this field based on their practical experiences in managing large amounts of
  data, and with far fewer assumptions- the data can be of any type, structured
  or unstructured, and may be extremely large. Outlier Analysis is a
  comprehensive exposition, as understood by data mining experts, statisticians
  and computer scientists. The book has been organized carefully, and emphasis
  was placed on simplifying the content, so that students and practitioners can
  also benefit. Chapters will typically cover one of three areas: methods and
  techniques commonly used in outlier analysis, such as linear methods,
  proximity-based methods, subspace methods, and supervised methods; data
  domains, such as, text, categorical, mixed-attribute, time-series, streaming,
  discrete sequence, spatial and network data; and key applications of these
  methods as applied to diverse domains such as credit card fraud detection,
  intrusion detection, medical diagnosis, earth science, web log analytics, and
  social network analysis are covered.%
    }
    \verb{doi}
    \verb 10.1007/978-1-4614-6396-2
    \endverb
    \field{isbn}{9781461463962}
    \field{pages}{1\bibrangedash 446}
    \field{title}{Outlier analysis}
    \field{volume}{9781461463}
    \field{journaltitle}{Outlier Analysis}
    \field{year}{2013}
  \endentry

  \entry{Alansari2018}{inproceedings}{}
    \name{author}{4}{}{%
      {{hash=AZ}{%
         family={Alansari},
         familyi={A\bibinitperiod},
         given={Zainab},
         giveni={Z\bibinitperiod},
      }}%
      {{hash=SS}{%
         family={Soomro},
         familyi={S\bibinitperiod},
         given={Safeeullah},
         giveni={S\bibinitperiod},
      }}%
      {{hash=BMR}{%
         family={Belgaum},
         familyi={B\bibinitperiod},
         given={Mohammad\bibnamedelima Riyaz},
         giveni={M\bibinitperiod\bibinitdelim R\bibinitperiod},
      }}%
      {{hash=SS}{%
         family={Shamshirband},
         familyi={S\bibinitperiod},
         given={Shahaboddin},
         giveni={S\bibinitperiod},
      }}%
    }
    \keyw{Big data,Fuzzy-AHP,Healthcare,Internet of Things (IoT)}
    \strng{namehash}{AZ+1}
    \strng{fullhash}{AZSSBMRSS1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{labelyear}{2018}
    \field{labeldatesource}{}
    \field{sortinit}{A}
    \field{sortinithash}{A}
    \field{abstract}{%
    Health is one of the sustainable development areas in all of the countries.
  Internet of Things has a variety of use in this sector which was not studied
  yet. The aim of this research is to prioritize IoT usage in the healthcare
  sector to achieve sustainable development. The study is an applied
  descriptive research according to data collection. As per the research
  methodology which is FAHP, it is a single cross-sectional survey research.
  After data collection, the agreed paired comparison matrices, allocated to
  weighted criteria and the priority of IoT usage were determined. Based on the
  research findings, the two criteria of “Economic Prosperity” and
  “Quality of Life” achieved the highest priority for IoT sustainable
  development in the healthcare sector. Moreover, the top priorities for IoT in
  the area of health, according to the usage, were identified as “Ultraviolet
  Radiation,” “Dental Health,” and “Fall Detection.”.%
    }
    \verb{doi}
    \verb 10.1007/978-981-10-6875-1_66
    \endverb
    \field{isbn}{9789811068744}
    \field{issn}{21945357}
    \field{pages}{675\bibrangedash 685}
    \field{title}{The rise of Internet of Things (IoT) in big healthcare data:
  Review and open research issues}
    \field{volume}{564}
    \field{journaltitle}{Advances in Intelligent Systems and Computing}
    \field{year}{2018}
  \endentry

  \entry{Chandola2009}{article}{}
    \name{author}{2}{}{%
      {{hash=ACBV}{%
         family={Arindam\bibnamedelima Chandola},
         familyi={A\bibinitperiod\bibinitdelim C\bibinitperiod},
         suffix={Varun},
         suffixi={V\bibinitperiod},
         given={Banerjee},
         giveni={B\bibinitperiod},
      }}%
      {{hash=KV}{%
         family={Kumar},
         familyi={K\bibinitperiod},
         given={Vipin},
         giveni={V\bibinitperiod},
      }}%
    }
    \strng{namehash}{ACVBKV1}
    \strng{fullhash}{ACVBKV1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{labelyear}{2009}
    \field{labeldatesource}{}
    \field{sortinit}{A}
    \field{sortinithash}{A}
    \field{issue}{3}
    \field{pages}{1\bibrangedash 58}
    \field{title}{Anomaly detection: A survey}
    \field{volume}{41}
    \field{journaltitle}{ACM computing surveys (CSUR)}
    \field{year}{2009}
  \endentry

  \entry{Braei2020}{misc}{}
    \name{author}{2}{}{%
      {{hash=BM}{%
         family={Braei},
         familyi={B\bibinitperiod},
         given={Mohammad},
         giveni={M\bibinitperiod},
      }}%
      {{hash=WS}{%
         family={Wagner},
         familyi={W\bibinitperiod},
         given={Sebastian},
         giveni={S\bibinitperiod},
      }}%
    }
    \strng{namehash}{BMWS1}
    \strng{fullhash}{BMWS1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{labelyear}{2020}
    \field{labeldatesource}{}
    \field{sortinit}{B}
    \field{sortinithash}{B}
    \field{abstract}{%
    Anomaly detection for time-series data has been an important research field
  for a long time. Seminal work on anomaly detection methods has been focussing
  on statistical approaches. In recent years an increasing number of machine
  learning algorithms have been developed to detect anomalies on time-series.
  Subsequently, researchers tried to improve these techniques using (deep)
  neural networks. In the light of the increasing number of anomaly detection
  methods, the body of research lacks a broad comparative evaluation of
  statistical, machine learning and deep learning methods. This paper studies
  20 univariate anomaly detection methods from the all three categories. The
  evaluation is conducted on publicly available datasets, which serve as
  benchmarks for time-series anomaly detection. By analyzing the accuracy of
  each method as well as the computation time of the algorithms, we provide a
  thorough insight about the performance of these anomaly detection approaches,
  alongside some general notion of which method is suited for a certain type of
  data.%
    }
    \field{issn}{23318422}
    \field{title}{Anomaly detection in univariate time-series: A survey on the
  state-of-the-art}
    \field{journaltitle}{arXiv}
    \field{year}{2020}
  \endentry

  \entry{DennyBritz2015}{misc}{}
    \name{author}{1}{}{%
      {{hash=BD}{%
         family={Britz},
         familyi={B\bibinitperiod},
         given={Denny},
         giveni={D\bibinitperiod},
      }}%
    }
    \strng{namehash}{BD1}
    \strng{fullhash}{BD1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{labelyear}{2015}
    \field{labeldatesource}{}
    \field{sortinit}{B}
    \field{sortinithash}{B}
    \field{title}{Implementing a Neural Network from Scratch in Python – An
  Introduction}
    \verb{url}
    \verb http://www.wildml.com/2015/09/implementing-a-neural-network-from-scra
    \verb tch/
    \endverb
    \field{journaltitle}{WILDML Artificial Intelligence, Deep Learning, and
  NLP}
    \field{month}{09}
    \field{year}{2015}
  \endentry

  \entry{JunyoungChung2014}{article}{}
    \name{author}{4}{}{%
      {{hash=CJ}{%
         family={Chung},
         familyi={C\bibinitperiod},
         given={Junyoung},
         giveni={J\bibinitperiod},
      }}%
      {{hash=GC}{%
         family={Gulcehre},
         familyi={G\bibinitperiod},
         given={Caglar},
         giveni={C\bibinitperiod},
      }}%
      {{hash=CK}{%
         family={Cho},
         familyi={C\bibinitperiod},
         given={KyungHyun},
         giveni={K\bibinitperiod},
      }}%
      {{hash=BY}{%
         family={Bengio},
         familyi={B\bibinitperiod},
         given={Yoshua},
         giveni={Y\bibinitperiod},
      }}%
    }
    \strng{namehash}{CJ+1}
    \strng{fullhash}{CJGCCKBY1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{labelyear}{2014}
    \field{labeldatesource}{}
    \field{sortinit}{C}
    \field{sortinithash}{C}
    \field{abstract}{%
    In this paper we compare different types of recurrent units in recurrent
  neural networks (RNNs). Especially, we focus on more sophisticated units that
  implement a gating mechanism, such as a long short-term memory (LSTM) unit
  and a recently proposed gated recurrent unit (GRU). We evaluate these
  recurrent units on the tasks of polyphonic music modeling and speech signal
  modeling. Our experiments revealed that these advanced recurrent units are
  indeed better than more traditional recurrent units such as tanh units. Also,
  we found GRU to be comparable to LSTM.%
    }
    \field{title}{Empirical Evaluation of Gated Recurrent Neural Networks on
  Sequence Modeling}
    \verb{url}
    \verb http://arxiv.org/abs/1412.3555
    \endverb
    \field{journaltitle}{NIPS 2014 Deep Learning and Representation Learning
  Workshop}
    \field{month}{12}
    \field{year}{2014}
  \endentry

  \entry{Dutta2018}{inproceedings}{}
    \name{author}{4}{}{%
      {{hash=DK}{%
         family={Dutta},
         familyi={D\bibinitperiod},
         given={Kartik},
         giveni={K\bibinitperiod},
      }}%
      {{hash=KP}{%
         family={Krishnan},
         familyi={K\bibinitperiod},
         given={Praveen},
         giveni={P\bibinitperiod},
      }}%
      {{hash=MM}{%
         family={Mathew},
         familyi={M\bibinitperiod},
         given={Minesh},
         giveni={M\bibinitperiod},
      }}%
      {{hash=JCV}{%
         family={Jawahar},
         familyi={J\bibinitperiod},
         given={C.\bibnamedelima V.},
         giveni={C\bibinitperiod\bibinitdelim V\bibinitperiod},
      }}%
    }
    \keyw{CNN RNN network,Data augmentation,Handwriting recognition,Image
  pre-processing}
    \strng{namehash}{DK+1}
    \strng{fullhash}{DKKPMMJCV1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{labelyear}{2018}
    \field{labeldatesource}{}
    \field{sortinit}{D}
    \field{sortinithash}{D}
    \field{abstract}{%
    The success of deep learning based models have centered around recent
  architectures and the availability of large scale annotated data. In this
  work, we explore these two factors systematically for improving handwritten
  recognition for scanned off-line document images. We propose a modified
  CNN-RNN hybrid architecture with a major focus on effective training using:
  (i) efficient initialization of network using synthetic data for pretraining,
  (ii) image normalization for slant correction and (iii) domain specific data
  transformation and distortion for learning important invariances. We perform
  a detailed ablation study to analyze the contribution of individual modules
  and present state of art results for the task of unconstrained line and word
  recognition on popular datasets such as IAM, RIMES and GW.%
    }
    \verb{doi}
    \verb 10.1109/ICFHR-2018.2018.00023
    \endverb
    \field{isbn}{9781538658758}
    \field{issn}{21676453}
    \field{pages}{80\bibrangedash 85}
    \field{title}{Improving CNN-RNN hybrid networks for handwriting
  recognition}
    \field{volume}{2018-Augus}
    \field{journaltitle}{Proceedings of International Conference on Frontiers
  in Handwriting Recognition, ICFHR}
    \field{year}{2018}
  \endentry

  \entry{Fan2016}{inproceedings}{}
    \name{author}{4}{}{%
      {{hash=FY}{%
         family={Fan},
         familyi={F\bibinitperiod},
         given={Yin},
         giveni={Y\bibinitperiod},
      }}%
      {{hash=LX}{%
         family={Lu},
         familyi={L\bibinitperiod},
         given={Xiangju},
         giveni={X\bibinitperiod},
      }}%
      {{hash=LD}{%
         family={Li},
         familyi={L\bibinitperiod},
         given={Dian},
         giveni={D\bibinitperiod},
      }}%
      {{hash=LY}{%
         family={Liu},
         familyi={L\bibinitperiod},
         given={Yuanliu},
         giveni={Y\bibinitperiod},
      }}%
    }
    \keyw{3d convolutional network,Emotion recognition,Long short term memory
  network,Model fusion,Recurrent neural network}
    \strng{namehash}{FY+1}
    \strng{fullhash}{FYLXLDLY1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{labelyear}{2016}
    \field{labeldatesource}{}
    \field{sortinit}{F}
    \field{sortinithash}{F}
    \field{abstract}{%
    In this paper, we present a video-based emotion recognition system
  submitted to the EmotiW 2016 Challenge. The core module of this system is a
  hybrid network that combines recurrent neural network (RNN) and 3D
  convolutional networks (C3D) in a late-fusion fashion. RNN and C3D encode
  appearance and motion information in different ways. Specifically, RNN takes
  appearance features extracted by convolutional neural network (CNN) over
  individual video frames as input and encodes motion later, while C3D models
  appearance and motion of video simultaneously. Combined with an audio module,
  our system achieved a recognition accuracy of 59.02% without using any
  additional emotion-labeled video clips in training set, compared to 53.8% of
  the winner of EmotiW 2015. Extensive experiments show that combining RNN and
  C3D together can improve video-based emotion recognition noticeably.%
    }
    \verb{doi}
    \verb 10.1145/2993148.2997632
    \endverb
    \field{isbn}{9781450345569}
    \field{pages}{445\bibrangedash 450}
    \field{title}{Video-Based emotion recognition using CNN-RNN and C3D hybrid
  networks}
    \field{journaltitle}{ICMI 2016 - Proceedings of the 18th ACM International
  Conference on Multimodal Interaction}
    \field{year}{2016}
  \endentry

  \entry{Filonov2016}{article}{}
    \name{author}{3}{}{%
      {{hash=FP}{%
         family={Filonov},
         familyi={F\bibinitperiod},
         given={Pavel},
         giveni={P\bibinitperiod},
      }}%
      {{hash=LA}{%
         family={Lavrentyev},
         familyi={L\bibinitperiod},
         given={Andrey},
         giveni={A\bibinitperiod},
      }}%
      {{hash=VA}{%
         family={Vorontsov},
         familyi={V\bibinitperiod},
         given={Artem},
         giveni={A\bibinitperiod},
      }}%
    }
    \strng{namehash}{FPLAVA1}
    \strng{fullhash}{FPLAVA1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{labelyear}{2016}
    \field{labeldatesource}{}
    \field{sortinit}{F}
    \field{sortinithash}{F}
    \field{abstract}{%
    We adopted an approach based on an LSTM neural network to monitor and
  detect faults in industrial multivariate time series data. To validate the
  approach we created a Modelica model of part of a real gasoil plant. By
  introducing hacks into the logic of the Modelica model, we were able to
  generate both the roots and causes of fault behavior in the plant. Having a
  self-consistent data set with labeled faults, we used an LSTM architecture
  with a forecasting error threshold to obtain precision and recall quality
  metrics. The dependency of the quality metric on the threshold level is
  considered. An appropriate mechanism such as "one handle" was introduced for
  filtering faults that are outside of the plant operator field of interest.%
    }
    \field{title}{Multivariate Industrial Time Series with Cyber-Attack
  Simulation: Fault Detection Using an LSTM-based Predictive Data Model}
    \field{month}{12}
    \field{year}{2016}
  \endentry

  \entry{Foorthuis2021}{article}{}
    \name{author}{1}{}{%
      {{hash=FR}{%
         family={Foorthuis},
         familyi={F\bibinitperiod},
         given={Ralph},
         giveni={R\bibinitperiod},
      }}%
    }
    \strng{namehash}{FR1}
    \strng{fullhash}{FR1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{labelyear}{2021}
    \field{labeldatesource}{}
    \field{sortinit}{F}
    \field{sortinithash}{F}
    \field{abstract}{%
    Anomalies are occurrences in a dataset that are in some way unusual and do
  not fit the general patterns. The concept of the anomaly is typically ill
  defined and perceived as vague and domain-dependent. Moreover, despite some
  250 years of publications on the topic, no comprehensive and concrete
  overviews of the different types of anomalies have hitherto been published.
  By means of an extensive literature review this study therefore offers the
  first theoretically principled and domain-independent typology of data
  anomalies and presents a full overview of anomaly types and subtypes. To
  concretely define the concept of the anomaly and its different
  manifestations, the typology employs five dimensions: data type, cardinality
  of relationship, anomaly level, data structure, and data distribution. These
  fundamental and data-centric dimensions naturally yield 3 broad groups, 9
  basic types, and 63 subtypes of anomalies. The typology facilitates the
  evaluation of the functional capabilities of anomaly detection algorithms,
  contributes to explainable data science, and provides insights into relevant
  topics such as local versus global anomalies.%
    }
    \verb{doi}
    \verb 10.1007/s41060-021-00265-1
    \endverb
    \field{issn}{2364-415X}
    \field{issue}{4}
    \field{title}{On the nature and types of anomalies: a review of deviations
  in data}
    \field{volume}{12}
    \field{journaltitle}{International Journal of Data Science and Analytics}
    \field{month}{10}
    \field{year}{2021}
  \endentry

  \entry{Gers2000}{article}{}
    \name{author}{3}{}{%
      {{hash=GFA}{%
         family={Gers},
         familyi={G\bibinitperiod},
         given={Felix\bibnamedelima A.},
         giveni={F\bibinitperiod\bibinitdelim A\bibinitperiod},
      }}%
      {{hash=SJ}{%
         family={Schmidhuber},
         familyi={S\bibinitperiod},
         given={Jürgen},
         giveni={J\bibinitperiod},
      }}%
      {{hash=CF}{%
         family={Cummins},
         familyi={C\bibinitperiod},
         given={Fred},
         giveni={F\bibinitperiod},
      }}%
    }
    \strng{namehash}{GFASJCF1}
    \strng{fullhash}{GFASJCF1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{labelyear}{2000}
    \field{labeldatesource}{}
    \field{sortinit}{G}
    \field{sortinithash}{G}
    \field{abstract}{%
    Long short-term memory (LSTM; Hochreiter and Schmidhuber, 1997) can solve
  numerous tasks not solvable by previous learning algorithms for recurrent
  neural networks (RNNs). We identify a weakness of LSTM networks processing
  continual input streams that are not a priori segmented into subsequences
  with explicitly marked ends at which the network's internal state could be
  reset. Without resets, the state may grow indefinitely and eventually cause
  the network to break down. Our remedy is a novel, adaptive "forget gate" that
  enables an LSTM cell to learn to reset itself at appropriate times, thus
  releasing internal resources. We review illustrative benchmark problems on
  which standard LSTM outperforms other RNN algorithms. All algorithms
  (including LSTM) fail to solve continual versions of these problems. LSTM
  with forget gates, however, easily solves them, and in an elegant way.%
    }
    \verb{doi}
    \verb 10.1162/089976600300015015
    \endverb
    \field{issn}{08997667}
    \field{issue}{10}
    \field{pages}{2451\bibrangedash 2471}
    \field{title}{Learning to forget: Continual prediction with LSTM}
    \field{volume}{12}
    \field{journaltitle}{Neural Computation}
    \field{year}{2000}
  \endentry

  \entry{Google2021}{misc}{}
    \name{author}{1}{}{%
      {{hash=G}{%
         family={Google},
         familyi={G\bibinitperiod},
      }}%
    }
    \strng{namehash}{G1}
    \strng{fullhash}{G1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{labelyear}{2021}
    \field{labeldatesource}{}
    \field{sortinit}{G}
    \field{sortinithash}{G}
    \field{title}{ROC and AUC}
    \verb{url}
    \verb https://developers.google.com/machine-learning/crash-course/classific
    \verb ation/roc-and-auc
    \endverb
    \field{year}{2021}
  \endentry

  \entry{Graves2005}{inproceedings}{}
    \name{author}{3}{}{%
      {{hash=GA}{%
         family={Graves},
         familyi={G\bibinitperiod},
         given={Alex},
         giveni={A\bibinitperiod},
      }}%
      {{hash=FS}{%
         family={Fernández},
         familyi={F\bibinitperiod},
         given={Santiago},
         giveni={S\bibinitperiod},
      }}%
      {{hash=SJ}{%
         family={Schmidhuber},
         familyi={S\bibinitperiod},
         given={Jürgen},
         giveni={J\bibinitperiod},
      }}%
    }
    \strng{namehash}{GAFSSJ1}
    \strng{fullhash}{GAFSSJ1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{labelyear}{2005}
    \field{labeldatesource}{}
    \field{sortinit}{G}
    \field{sortinithash}{G}
    \field{abstract}{%
    In this paper, we carry out two experiments on the TIMIT speech corpus with
  bidirectional and unidirectional Long Short Term Memory (LSTM) networks. In
  the first experiment (framewise phoneme classification) we find that
  bidirectional LSTM outperforms both unidirectional LSTM and conventional
  Recurrent Neural Networks (RNNs). In the second (phoneme recognition) we find
  that a hybrid BLSTM-HMM system improves on an equivalent traditional HMM
  system, as well as unidirectional LSTM-HMM. © Springer-Verlag Berlin
  Heidelberg 2005.%
    }
    \verb{doi}
    \verb 10.1007/11550907_126
    \endverb
    \field{issn}{03029743}
    \field{title}{Bidirectional LSTM networks for improved phoneme
  classification and recognition}
    \field{volume}{3697 LNCS}
    \field{journaltitle}{Lecture Notes in Computer Science (including subseries
  Lecture Notes in Artificial Intelligence and Lecture Notes in
  Bioinformatics)}
    \field{year}{2005}
  \endentry

  \entry{Hauskrecht2007}{article}{}
    \name{author}{5}{}{%
      {{hash=HM}{%
         family={Hauskrecht},
         familyi={H\bibinitperiod},
         given={Milos},
         giveni={M\bibinitperiod},
      }}%
      {{hash=VM}{%
         family={Valko},
         familyi={V\bibinitperiod},
         given={Michal},
         giveni={M\bibinitperiod},
      }}%
      {{hash=KB}{%
         family={Kveton},
         familyi={K\bibinitperiod},
         given={Branislav},
         giveni={B\bibinitperiod},
      }}%
      {{hash=VS}{%
         family={Visweswaran},
         familyi={V\bibinitperiod},
         given={Shyam},
         giveni={S\bibinitperiod},
      }}%
      {{hash=CGF}{%
         family={Cooper},
         familyi={C\bibinitperiod},
         given={Gregory\bibnamedelima F.},
         giveni={G\bibinitperiod\bibinitdelim F\bibinitperiod},
      }}%
    }
    \strng{namehash}{HM+1}
    \strng{fullhash}{HMVMKBVSCGF1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{labelyear}{2007}
    \field{labeldatesource}{}
    \field{sortinit}{H}
    \field{sortinithash}{H}
    \field{abstract}{%
    Anomaly detection methods can be very useful in identifying interesting or
  concerning events. In this work, we develop and examine new probabilistic
  anomaly detection methods that let us evaluate management decisions for a
  specific patient and identify those decisions that are highly unusual with
  respect to patients with the same or similar condition. The statistics used
  in this detection are derived from probabilistic models such as Bayesian
  networks that are learned from a database of past patient cases. We evaluate
  our methods on the problem of detection of unusual hospitalization patterns
  for patients with community acquired pneumonia. The results show very
  encouraging detection performance with 0.5 precision at 0.53 recall and give
  us hope that these techniques may provide the basis of intelligent monitoring
  systems that alert clinicians to the occurrence of unusual events or
  decisions.%
    }
    \field{issn}{15594076}
    \field{pages}{319\bibrangedash 323}
    \field{title}{Evidence-based anomaly detection in clinical domains.}
    \field{journaltitle}{AMIA ... Annual Symposium proceedings / AMIA
  Symposium. AMIA Symposium}
    \field{year}{2007}
  \endentry

  \entry{Hochreiter1997}{article}{}
    \name{author}{2}{}{%
      {{hash=HS}{%
         family={Hochreiter},
         familyi={H\bibinitperiod},
         given={Sepp},
         giveni={S\bibinitperiod},
      }}%
      {{hash=SJ}{%
         family={Schmidhuber},
         familyi={S\bibinitperiod},
         given={Jürgen},
         giveni={J\bibinitperiod},
      }}%
    }
    \strng{namehash}{HSSJ1}
    \strng{fullhash}{HSSJ1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{labelyear}{1997}
    \field{labeldatesource}{}
    \field{sortinit}{H}
    \field{sortinithash}{H}
    \field{abstract}{%
    Learning to store information over extended time intervals by recurrent
  backpropagation takes a very long time, mostly because of insufficient,
  decaying error backflow. We briefly review Hochreiter's (1991) analysis of
  this problem, then address it by introducing a novel, efficient, gradient
  based method called long short-term memory (LSTM). Truncating the gradient
  where this does not do harm, LSTM can learn to bridge minimal time lags in
  excess of 1000 discrete-time steps by enforcing constant error flow through
  constant error carousels within special units. Multiplicative gate units
  learn to open and close access to the constant error flow. LSTM is local in
  space and time; its computational complexity per time step and weight is O.
  1. Our experiments with artificial data involve local, distributed,
  real-valued, and noisy pattern representations. In comparisons with real-time
  recurrent learning, back propagation through time, recurrent cascade
  correlation, Elman nets, and neural sequence chunking, LSTM leads to many
  more successful runs, and learns much faster. LSTM also solves complex,
  artificial long-time-lag tasks that have never been solved by previous
  recurrent network algorithms.%
    }
    \verb{doi}
    \verb 10.1162/neco.1997.9.8.1735
    \endverb
    \field{issn}{0899-7667}
    \field{issue}{8}
    \field{pages}{1735\bibrangedash 1780}
    \field{title}{Long Short-Term Memory}
    \verb{url}
    \verb https://direct.mit.edu/neco/article/9/8/1735-1780/6109
    \endverb
    \field{volume}{9}
    \field{journaltitle}{Neural Computation}
    \field{month}{11}
    \field{year}{1997}
  \endentry

  \entry{Hodge2004}{misc}{}
    \name{author}{2}{}{%
      {{hash=HVJ}{%
         family={Hodge},
         familyi={H\bibinitperiod},
         given={Victoria\bibnamedelima J.},
         giveni={V\bibinitperiod\bibinitdelim J\bibinitperiod},
      }}%
      {{hash=AJ}{%
         family={Austin},
         familyi={A\bibinitperiod},
         given={Jim},
         giveni={J\bibinitperiod},
      }}%
    }
    \keyw{Anomaly,Detection,Deviation,Noise,Novelty,Outlier,Recognition}
    \strng{namehash}{HVJAJ1}
    \strng{fullhash}{HVJAJ1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{labelyear}{2004}
    \field{labeldatesource}{}
    \field{sortinit}{H}
    \field{sortinithash}{H}
    \field{abstract}{%
    Outlier detection has been used for centuries to detect and, where
  appropriate, remove anomalous observations from data. Outliers arise due to
  mechanical faults, changes in system behaviour, fraudulent behaviour, human
  error, instrument error or simply through natural deviations in populations.
  Their detection can identify system faults and fraud before they escalate
  with potentially catastrophic consequences. It can identify errors and remove
  their contaminating effect on the data set and as such to purify the data for
  processing. The original outlier detection methods were arbitrary but now,
  principled and systematic techniques are used, drawn from the full gamut of
  Computer Science and Statistics. In this paper, we introduce a survey of
  contemporary techniques for outlier detection. We identify their respective
  motivations and distinguish their advantages and disadvantages in a
  comparative review.%
    }
    \verb{doi}
    \verb 10.1023/B:AIRE.0000045502.10941.a9
    \endverb
    \field{issn}{02692821}
    \field{issue}{2}
    \field{pages}{85\bibrangedash 126}
    \field{title}{A survey of outlier detection methodologies}
    \field{volume}{22}
    \field{journaltitle}{Artificial Intelligence Review}
    \field{year}{2004}
  \endentry

  \entry{Cicek2016}{inproceedings}{}
    \name{author}{5}{}{%
      {{hash=zi}{%
         prefix={Özgün},
         prefixi={z\bibinitperiod},
         family={Çiçek},
         familyi={i\bibinitperiod},
      }}%
      {{hash=AA}{%
         family={Abdulkadir},
         familyi={A\bibinitperiod},
         given={Ahmed},
         giveni={A\bibinitperiod},
      }}%
      {{hash=LSS}{%
         family={Lienkamp},
         familyi={L\bibinitperiod},
         given={Soeren\bibnamedelima S.},
         giveni={S\bibinitperiod\bibinitdelim S\bibinitperiod},
      }}%
      {{hash=BT}{%
         family={Brox},
         familyi={B\bibinitperiod},
         given={Thomas},
         giveni={T\bibinitperiod},
      }}%
      {{hash=RO}{%
         family={Ronneberger},
         familyi={R\bibinitperiod},
         given={Olaf},
         giveni={O\bibinitperiod},
      }}%
    }
    \strng{namehash}{iz+1}
    \strng{fullhash}{izAALSSBTRO1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{labelyear}{2016}
    \field{labeldatesource}{}
    \field{sortinit}{I}
    \field{sortinithash}{I}
    \field{abstract}{%
    This paper introduces a network for volumetric segmentation that learns
  from sparsely annotated volumetric images. We outline two attractive use
  cases of this method: (1) In a semi-automated setup,the user annotates some
  slices in the volume to be segmented. The network learns from these sparse
  annotations and provides a dense 3D segmentation. (2) In a fully-automated
  setup,we assume that a representative,sparsely annotated training set exists.
  Trained on this data set,the network densely segments new volumetric images.
  The proposed network extends the previous u-net architecture from Ronneberger
  et al. by replacing all 2D operations with their 3D counterparts. The
  implementation performs on-the-fly elastic deformations for efficient data
  augmentation during training. It is trained end-to-end from scratch,i.e.,no
  pre-trained network is required. We test the performance of the proposed
  method on a complex,highly variable 3D structure,the Xenopus kidney,and
  achieve good results for both use cases.%
    }
    \verb{doi}
    \verb 10.1007/978-3-319-46723-8_49
    \endverb
    \field{issn}{16113349}
    \field{title}{3D U-net: Learning dense volumetric segmentation from sparse
  annotation}
    \field{volume}{9901 LNCS}
    \field{journaltitle}{Lecture Notes in Computer Science (including subseries
  Lecture Notes in Artificial Intelligence and Lecture Notes in
  Bioinformatics)}
    \field{year}{2016}
  \endentry

  \entry{Katanforoosh2019}{misc}{}
    \name{author}{3}{}{%
      {{hash=KK}{%
         family={Katanforoosh},
         familyi={K\bibinitperiod},
         given={Kian},
         giveni={K\bibinitperiod},
      }}%
      {{hash=KD}{%
         family={Kunin},
         familyi={K\bibinitperiod},
         given={Daniel},
         giveni={D\bibinitperiod},
      }}%
      {{hash=MJ}{%
         family={Ma},
         familyi={M\bibinitperiod},
         given={Jiaju},
         giveni={J\bibinitperiod},
      }}%
    }
    \strng{namehash}{KKKDMJ1}
    \strng{fullhash}{KKKDMJ1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{labelyear}{2019}
    \field{labeldatesource}{}
    \field{sortinit}{K}
    \field{sortinithash}{K}
    \field{title}{Parameter optimization in neural networks}
    \verb{url}
    \verb https://www.deeplearning.ai/ai-notes/optimization/
    \endverb
    \field{year}{2019}
  \endentry

  \entry{LeCun1998}{article}{}
    \name{author}{4}{}{%
      {{hash=LY}{%
         family={LeCun},
         familyi={L\bibinitperiod},
         given={Yann},
         giveni={Y\bibinitperiod},
      }}%
      {{hash=BL}{%
         family={Bottou},
         familyi={B\bibinitperiod},
         given={Léon},
         giveni={L\bibinitperiod},
      }}%
      {{hash=BY}{%
         family={Bengio},
         familyi={B\bibinitperiod},
         given={Yoshua},
         giveni={Y\bibinitperiod},
      }}%
      {{hash=HP}{%
         family={Haffner},
         familyi={H\bibinitperiod},
         given={Patrick},
         giveni={P\bibinitperiod},
      }}%
    }
    \keyw{Convolutional neural networks,Document recognition,Finite state
  transducers,Gradient-based learning,Graph transformer networks,Machine
  learning,Neural networks,Optical character recognition (OCR)}
    \strng{namehash}{LY+1}
    \strng{fullhash}{LYBLBYHP1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{labelyear}{1998}
    \field{labeldatesource}{}
    \field{sortinit}{L}
    \field{sortinithash}{L}
    \field{abstract}{%
    Multilayer neural networks trained with the back-propagation algorithm
  constitute the best example of a successful gradient-based learning
  technique. Given an appropriate network architecture, gradient-based learning
  algorithms can be used to synthesize a complex decision surface that can
  classify high-dimensional patterns, such as handwritten characters, with
  minimal preprocessing. This paper reviews various methods applied to
  handwritten character recognition and compares them on a standard handwritten
  digit recognition task. Convolutional neural networks, which are specifically
  designed to deal with the variability of two dimensional (2-D) shapes, are
  shown to outperform all other techniques. Real-life document recognition
  systems are composed of multiple modules including field extraction,
  segmentation, recognition, and language modeling. A new learning paradigm,
  called graph transformer networks (GTN's), allows such multimodule systems to
  be trained globally using gradient-based methods so as to minimize an overall
  performance measure. Two systems for online handwriting recognition are
  described. Experiments demonstrate the advantage of global training, and the
  flexibility of graph transformer networks. A graph transformer network for
  reading a bank check is also described. It uses convolutional neural network
  character recognizers combined with global training techniques to provide
  record accuracy on business and personal checks. It is deployed commercially
  and reads several million checks per day. © 1998 IEEE.%
    }
    \verb{doi}
    \verb 10.1109/5.726791
    \endverb
    \field{issn}{00189219}
    \field{issue}{11}
    \field{pages}{2278\bibrangedash 2323}
    \field{title}{Gradient-based learning applied to document recognition}
    \field{volume}{86}
    \field{journaltitle}{Proceedings of the IEEE}
    \field{year}{1998}
  \endentry

  \entry{Lendave2021}{misc}{}
    \name{author}{1}{}{%
      {{hash=LV}{%
         family={Lendave},
         familyi={L\bibinitperiod},
         given={Vijaysinh},
         giveni={V\bibinitperiod},
      }}%
    }
    \strng{namehash}{LV1}
    \strng{fullhash}{LV1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{labelyear}{2021}
    \field{labeldatesource}{}
    \field{sortinit}{L}
    \field{sortinithash}{L}
    \field{title}{LSTM Vs GRU in Recurrent Neural Network: A Comparative Study}
    \verb{url}
    \verb https://analyticsindiamag.com/lstm-vs-gru-in-recurrent-neural-network
    \verb -a-comparative-study/
    \endverb
    \field{month}{08}
    \field{year}{2021}
  \endentry

  \entry{Malhotra2015}{inproceedings}{}
    \name{author}{4}{}{%
      {{hash=MP}{%
         family={Malhotra},
         familyi={M\bibinitperiod},
         given={Pankaj},
         giveni={P\bibinitperiod},
      }}%
      {{hash=VL}{%
         family={Vig},
         familyi={V\bibinitperiod},
         given={Lovekesh},
         giveni={L\bibinitperiod},
      }}%
      {{hash=SG}{%
         family={Shroff},
         familyi={S\bibinitperiod},
         given={Gautam},
         giveni={G\bibinitperiod},
      }}%
      {{hash=AP}{%
         family={Agarwal},
         familyi={A\bibinitperiod},
         given={Puneet},
         giveni={P\bibinitperiod},
      }}%
    }
    \strng{namehash}{MP+1}
    \strng{fullhash}{MPVLSGAP1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{labelyear}{2015}
    \field{labeldatesource}{}
    \field{sortinit}{M}
    \field{sortinithash}{M}
    \field{abstract}{%
    Long Short Term Memory (LSTM) networks have been demonstrated to be
  particularly useful for learning sequences containing longer term patterns of
  unknown length, due to their ability to maintain long term memory. Stacking
  recurrent hidden layers in such networks also enables the learning of higher
  level temporal features, for faster learning with sparser representations. In
  this paper, we use stacked LSTM networks for anomaly/fault detection in time
  series. A network is trained on non-anomalous data and used as a predictor
  over a number of time steps. The resulting prediction errors are modeled as a
  multivariate Gaussian distribution, which is used to assess the likelihood of
  anomalous behavior. The efficacy of this approach is demonstrated on four
  datasets: ECG, space shuttle, power demand, and multi-sensor engine dataset.%
    }
    \field{isbn}{9782875870148}
    \field{pages}{89\bibrangedash 94}
    \field{title}{Long Short Term Memory networks for anomaly detection in time
  series}
    \field{journaltitle}{23rd European Symposium on Artificial Neural Networks,
  Computational Intelligence and Machine Learning, ESANN 2015 - Proceedings}
    \field{year}{2015}
  \endentry

  \entry{Munir2019}{article}{}
    \name{author}{4}{}{%
      {{hash=MM}{%
         family={Munir},
         familyi={M\bibinitperiod},
         given={Mohsin},
         giveni={M\bibinitperiod},
      }}%
      {{hash=SSA}{%
         family={Siddiqui},
         familyi={S\bibinitperiod},
         given={Shoaib\bibnamedelima Ahmed},
         giveni={S\bibinitperiod\bibinitdelim A\bibinitperiod},
      }}%
      {{hash=DA}{%
         family={Dengel},
         familyi={D\bibinitperiod},
         given={Andreas},
         giveni={A\bibinitperiod},
      }}%
      {{hash=AS}{%
         family={Ahmed},
         familyi={A\bibinitperiod},
         given={Sheraz},
         giveni={S\bibinitperiod},
      }}%
    }
    \keyw{Anomaly detection,artificial intelligence,convolutional neural
  network,deep neural networks,recurrent neural networks,time series analysis}
    \strng{namehash}{MM+1}
    \strng{fullhash}{MMSSADAAS1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{labelyear}{2019}
    \field{labeldatesource}{}
    \field{sortinit}{M}
    \field{sortinithash}{M}
    \field{abstract}{%
    Traditional distance and density-based anomaly detection techniques are
  unable to detect periodic and seasonality related point anomalies which occur
  commonly in streaming data, leaving a big gap in time series anomaly
  detection in the current era of the IoT. To address this problem, we present
  a novel deep learning-based anomaly detection approach (DeepAnT) for time
  series data, which is equally applicable to the non-streaming cases. DeepAnT
  is capable of detecting a wide range of anomalies, i.e., point anomalies,
  contextual anomalies, and discords in time series data. In contrast to the
  anomaly detection methods where anomalies are learned, DeepAnT uses unlabeled
  data to capture and learn the data distribution that is used to forecast the
  normal behavior of a time series. DeepAnT consists of two modules: time
  series predictor and anomaly detector. The time series predictor module uses
  deep convolutional neural network (CNN) to predict the next time stamp on the
  defined horizon. This module takes a window of time series (used as a
  context) and attempts to predict the next time stamp. The predicted value is
  then passed to the anomaly detector module, which is responsible for tagging
  the corresponding time stamp as normal or abnormal. DeepAnT can be trained
  even without removing the anomalies from the given data set. Generally, in
  deep learning-based approaches, a lot of data are required to train a model.
  Whereas in DeepAnT, a model can be trained on relatively small data set while
  achieving good generalization capabilities due to the effective parameter
  sharing of the CNN. As the anomaly detection in DeepAnT is unsupervised, it
  does not rely on anomaly labels at the time of model generation. Therefore,
  this approach can be directly applied to real-life scenarios where it is
  practically impossible to label a big stream of data coming from
  heterogeneous sensors comprising of both normal as well as anomalous points.
  We have performed a detailed evaluation of 15 algorithms on 10 anomaly
  detection benchmarks, which contain a total of 433 real and synthetic time
  series. Experiments show that DeepAnT outperforms the state-of-the-art
  anomaly detection methods in most of the cases, while performing on par with
  others.%
    }
    \verb{doi}
    \verb 10.1109/ACCESS.2018.2886457
    \endverb
    \field{issn}{21693536}
    \field{pages}{1991\bibrangedash 2005}
    \field{title}{DeepAnT: A Deep Learning Approach for Unsupervised Anomaly
  Detection in Time Series}
    \field{volume}{7}
    \field{journaltitle}{IEEE Access}
    \field{year}{2019}
  \endentry

  \entry{Ord1996}{article}{}
    \name{author}{1}{}{%
      {{hash=OK}{%
         family={Ord},
         familyi={O\bibinitperiod},
         given={Keith},
         giveni={K\bibinitperiod},
      }}%
    }
    \strng{namehash}{OK1}
    \strng{fullhash}{OK1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{labelyear}{1996}
    \field{labeldatesource}{}
    \field{sortinit}{O}
    \field{sortinithash}{O}
    \field{abstract}{%
    No abstract is available for this item.%
    }
    \field{issue}{1}
    \field{title}{Outliers in statistical data : V. Barnett and T. Lewis, 1994,
  3rd edition, (John Wiley and Sons, Chichester), 584 pp., [UK pound]55.00,
  ISBN 0-471-93094-6}
    \field{volume}{12}
    \field{journaltitle}{International Journal of Forecasting}
    \field{year}{1996}
  \endentry

  \entry{MichaelPhi2018}{misc}{}
    \name{author}{1}{}{%
      {{hash=PM}{%
         family={Phi},
         familyi={P\bibinitperiod},
         given={Michael},
         giveni={M\bibinitperiod},
      }}%
    }
    \strng{namehash}{PM1}
    \strng{fullhash}{PM1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{labelyear}{2018}
    \field{labeldatesource}{}
    \field{sortinit}{P}
    \field{sortinithash}{P}
    \field{title}{Illustrated Guide to LSTM’s and GRU’s: A step by step
  explanation}
    \verb{url}
    \verb https://towardsdatascience.com/illustrated-guide-to-lstms-and-gru-s-a
    \verb -step-by-step-explanation-44e9eb85bf21
    \endverb
    \field{journaltitle}{Towards Data Science}
    \field{month}{09}
    \field{year}{2018}
  \endentry

  \entry{Ronneberger2015}{inproceedings}{}
    \name{author}{3}{}{%
      {{hash=RO}{%
         family={Ronneberger},
         familyi={R\bibinitperiod},
         given={Olaf},
         giveni={O\bibinitperiod},
      }}%
      {{hash=FP}{%
         family={Fischer},
         familyi={F\bibinitperiod},
         given={Philipp},
         giveni={P\bibinitperiod},
      }}%
      {{hash=BT}{%
         family={Brox},
         familyi={B\bibinitperiod},
         given={Thomas},
         giveni={T\bibinitperiod},
      }}%
    }
    \strng{namehash}{ROFPBT1}
    \strng{fullhash}{ROFPBT1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{labelyear}{2015}
    \field{labeldatesource}{}
    \field{sortinit}{R}
    \field{sortinithash}{R}
    \field{abstract}{%
    There is large consent that successful training of deep networks requires
  many thousand annotated training samples. In this paper, we present a network
  and training strategy that relies on the strong use of data augmentation to
  use the available annotated samples more efficiently. The architecture
  consists of a contracting path to capture context and a symmetric expanding
  path that enables precise localization. We show that such a network can be
  trained end-to-end from very few images and outperforms the prior best method
  (a sliding-window convolutional network) on the ISBI challenge for
  segmentation of neuronal structures in electron microscopic stacks. Using the
  same network trained on transmitted light microscopy images (phase contrast
  and DIC) we won the ISBI cell tracking challenge 2015 in these categories by
  a large margin. Moreover, the network is fast. Segmentation of a 512x512
  image takes less than a second on a recent GPU. The full implementation
  (based on Caffe) and the trained networks are available at
  http://lmb.informatik.uni-freiburg.de/people/ronneber/u-net.%
    }
    \verb{doi}
    \verb 10.1007/978-3-319-24574-4_28
    \endverb
    \field{isbn}{9783319245737}
    \field{issn}{16113349}
    \field{pages}{234\bibrangedash 241}
    \field{title}{U-net: Convolutional networks for biomedical image
  segmentation}
    \field{volume}{9351}
    \field{journaltitle}{Lecture Notes in Computer Science (including subseries
  Lecture Notes in Artificial Intelligence and Lecture Notes in
  Bioinformatics)}
    \field{year}{2015}
  \endentry

  \entry{Rosenblatt1958}{article}{}
    \name{author}{1}{}{%
      {{hash=RF}{%
         family={Rosenblatt},
         familyi={R\bibinitperiod},
         given={F.},
         giveni={F\bibinitperiod},
      }}%
    }
    \strng{namehash}{RF1}
    \strng{fullhash}{RF1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{labelyear}{1958}
    \field{labeldatesource}{}
    \field{sortinit}{R}
    \field{sortinithash}{R}
    \verb{doi}
    \verb 10.1037/h0042519
    \endverb
    \field{issn}{1939-1471}
    \field{issue}{6}
    \field{title}{The perceptron: A probabilistic model for information storage
  and organization in the brain.}
    \field{volume}{65}
    \field{journaltitle}{Psychological Review}
    \field{year}{1958}
  \endentry

  \entry{RichStureborg2019}{misc}{}
    \name{author}{1}{}{%
      {{hash=SR}{%
         family={Stureborg},
         familyi={S\bibinitperiod},
         given={Rich},
         giveni={R\bibinitperiod},
      }}%
    }
    \strng{namehash}{SR1}
    \strng{fullhash}{SR1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{labelyear}{2019}
    \field{labeldatesource}{}
    \field{sortinit}{S}
    \field{sortinithash}{S}
    \field{pages}{\bibrangedash undefined}
    \field{title}{Conv Nets for dummies}
    \verb{url}
    \verb https://towardsdatascience.com/conv-nets-for-dummies-a-bottom-up-appr
    \verb oach-c1b754fb14d6
    \endverb
    \field{journaltitle}{Towards Data Science}
    \field{month}{01}
    \field{year}{2019}
  \endentry

  \entry{Thabtah2020}{article}{}
    \name{author}{4}{}{%
      {{hash=TF}{%
         family={Thabtah},
         familyi={T\bibinitperiod},
         given={Fadi},
         giveni={F\bibinitperiod},
      }}%
      {{hash=HS}{%
         family={Hammoud},
         familyi={H\bibinitperiod},
         given={Suhel},
         giveni={S\bibinitperiod},
      }}%
      {{hash=KF}{%
         family={Kamalov},
         familyi={K\bibinitperiod},
         given={Firuz},
         giveni={F\bibinitperiod},
      }}%
      {{hash=GA}{%
         family={Gonsalves},
         familyi={G\bibinitperiod},
         given={Amanda},
         giveni={A\bibinitperiod},
      }}%
    }
    \keyw{Class imbalance,Classification,Data analysis,Machine
  learning,Statistical analysis,Supervised learning}
    \strng{namehash}{TF+1}
    \strng{fullhash}{TFHSKFGA1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{labelyear}{2020}
    \field{labeldatesource}{}
    \field{sortinit}{T}
    \field{sortinithash}{T}
    \field{abstract}{%
    The advent of Big Data has ushered a new era of scientific breakthroughs.
  One of the common issues that affects raw data is class imbalance problem
  which refers to imbalanced distribution of values of the response variable.
  This issue is present in fraud detection, network intrusion detection,
  medical diagnostics, and a number of other fields where negatively labeled
  instances significantly outnumber positively labeled instances. Modern
  machine learning techniques struggle to deal with imbalanced data by focusing
  on minimizing the error rate for the majority class while ignoring the
  minority class. The goal of our paper is demonstrate the effects of class
  imbalance on classification models. Concretely, we study the impact of
  varying class imbalance ratios on classifier accuracy. By highlighting the
  precise nature of the relationship between the degree of class imbalance and
  the corresponding effects on classifier performance we hope to help
  researchers to better tackle the problem. To this end, we carry out extensive
  experiments using 10-fold cross validation on a large number of datasets. In
  particular, we determine that the relationship between the class imbalance
  ratio and the accuracy is convex.%
    }
    \verb{doi}
    \verb 10.1016/j.ins.2019.11.004
    \endverb
    \field{issn}{00200255}
    \field{pages}{429\bibrangedash 441}
    \field{title}{Data imbalance in classification: Experimental evaluation}
    \field{volume}{513}
    \field{journaltitle}{Information Sciences}
    \field{year}{2020}
  \endentry

  \entry{Verner2019}{thesis}{}
    \name{author}{1}{}{%
      {{hash=VA}{%
         family={Verner},
         familyi={V\bibinitperiod},
         given={Alexander},
         giveni={A\bibinitperiod},
      }}%
    }
    \strng{namehash}{VA1}
    \strng{fullhash}{VA1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{labelyear}{2019}
    \field{labeldatesource}{}
    \field{sortinit}{V}
    \field{sortinithash}{V}
    \field{pages}{1\bibrangedash 131}
    \field{title}{LSTM Networks for Detection and Classification of Anomalies
  in Raw Sensor Data}
    \verb{url}
    \verb https://nsuworks.nova.edu/gscis_etd/1074.
    \endverb
    \list{institution}{1}{%
      {Nova Southeastern University}%
    }
    \field{year}{2019}
  \endentry

  \entry{Wen2019}{misc}{}
    \name{author}{2}{}{%
      {{hash=WT}{%
         family={Wen},
         familyi={W\bibinitperiod},
         given={Tailai},
         giveni={T\bibinitperiod},
      }}%
      {{hash=KR}{%
         family={Keyes},
         familyi={K\bibinitperiod},
         given={Roy},
         giveni={R\bibinitperiod},
      }}%
    }
    \strng{namehash}{WTKR1}
    \strng{fullhash}{WTKR1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{labelyear}{2019}
    \field{labeldatesource}{}
    \field{sortinit}{W}
    \field{sortinithash}{W}
    \field{abstract}{%
    Time series anomaly detection plays a critical role in automated monitoring
  systems. Most previous deep learning efforts related to time series anomaly
  detection were based on recurrent neural networks (RNN). In this paper, we
  propose a time series segmentation approach based on convolutional neural
  networks (CNN) for anomaly detection. Moreover, we propose a transfer
  learning framework that pretrains a model on a large-scale synthetic
  univariate time series data set and then fine-tunes its weights on
  small-scale, univariate or multivariate data sets with previously unseen
  classes of anomalies. For the multivariate case we introduce a novel network
  architecture. The approach was tested on multiple synthetic and real data
  sets successfully.%
    }
    \field{issn}{23318422}
    \field{title}{Time Series Anomaly Detection Using Convolutional Neural
  Networks and Transfer Learning}
    \field{journaltitle}{arXiv}
    \field{year}{2019}
  \endentry

  \entry{Wu2020}{article}{}
    \name{author}{2}{}{%
      {{hash=WR}{%
         family={Wu},
         familyi={W\bibinitperiod},
         given={Renjie},
         giveni={R\bibinitperiod},
      }}%
      {{hash=KEJ}{%
         family={Keogh},
         familyi={K\bibinitperiod},
         given={Eamonn\bibnamedelima J.},
         giveni={E\bibinitperiod\bibinitdelim J\bibinitperiod},
      }}%
    }
    \strng{namehash}{WRKEJ1}
    \strng{fullhash}{WRKEJ1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{labelyear}{2020}
    \field{labeldatesource}{}
    \field{sortinit}{W}
    \field{sortinithash}{W}
    \field{title}{Current Time Series Anomaly Detection Benchmarks are Flawed
  and are Creating the Illusion of Progress}
    \field{year}{2020}
  \endentry

  \entry{Zheng2014}{inproceedings}{}
    \name{author}{5}{}{%
      {{hash=ZY}{%
         family={Zheng},
         familyi={Z\bibinitperiod},
         given={Yi},
         giveni={Y\bibinitperiod},
      }}%
      {{hash=LQ}{%
         family={Liu},
         familyi={L\bibinitperiod},
         given={Qi},
         giveni={Q\bibinitperiod},
      }}%
      {{hash=CE}{%
         family={Chen},
         familyi={C\bibinitperiod},
         given={Enhong},
         giveni={E\bibinitperiod},
      }}%
      {{hash=GY}{%
         family={Ge},
         familyi={G\bibinitperiod},
         given={Yong},
         giveni={Y\bibinitperiod},
      }}%
      {{hash=ZJL}{%
         family={Zhao},
         familyi={Z\bibinitperiod},
         given={J.\bibnamedelima Leon},
         giveni={J\bibinitperiod\bibinitdelim L\bibinitperiod},
      }}%
    }
    \strng{namehash}{ZY+1}
    \strng{fullhash}{ZYLQCEGYZJL1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{labelyear}{2014}
    \field{labeldatesource}{}
    \field{sortinit}{Z}
    \field{sortinithash}{Z}
    \field{abstract}{%
    Time series (particularly multivariate) classification has drawn a lot of
  attention in the literature because of its broad applications for different
  domains, such as health informatics and bioinformatics. Thus, many algorithms
  have been developed for this task. Among them, nearest neighbor
  classification (particularly 1-NN) combined with Dynamic Time Warping (DTW)
  achieves the state of the art performance. However, when data set grows
  larger, the time consumption of 1-NN with DTW grows linearly. Compared to
  1-NN with DTW, the traditional feature-based classification methods are
  usually more efficient but less effective since their performance is usually
  dependent on the quality of hand-crafted features. To that end, in this
  paper, we explore the feature learning techniques to improve the performance
  of traditional feature-based approaches. Specifically, we propose a novel
  deep learning framework for multivariate time series classification. We
  conduct two groups of experiments on real-world data sets from different
  application domains. The final results show that our model is not only more
  efficient than the state of the art but also competitive in accuracy. It also
  demonstrates that feature learning is worth to investigate for time series
  classification. © 2014 Springer International Publishing Switzerland.%
    }
    \verb{doi}
    \verb 10.1007/978-3-319-08010-9_33
    \endverb
    \field{isbn}{9783319080093}
    \field{issn}{16113349}
    \field{pages}{298\bibrangedash 310}
    \field{title}{Time series classification using multi-channels deep
  convolutional neural networks}
    \field{volume}{8485 LNCS}
    \field{journaltitle}{Lecture Notes in Computer Science (including subseries
  Lecture Notes in Artificial Intelligence and Lecture Notes in
  Bioinformatics)}
    \field{year}{2014}
  \endentry
\enddatalist
\endinput
