% Chapter 1

\chapter{Introduction} % Main chapter title

\label{1.} % For referencing the chapter elsewhere, use \ref{Chapter1} 

%----------------------------------------------------------------------------------------

% Define some commands to keep the formatting separated from the content 
\newcommand{\keyword}[1]{\textbf{#1}}
\newcommand{\tabhead}[1]{\textbf{#1}}
\newcommand{\code}[1]{\texttt{#1}}
\newcommand{\file}[1]{\texttt{\bfseries#1}}
\newcommand{\option}[1]{\texttt{\itshape#1}}


%----------------------------------------------------------------------------------------


With the rise of the Internet of Things (IoT) and ever more sensors, gadgets and smart devices like smartwatches for fall detection or blood pressure monitoring, or fridges for temperature protective control in use, the amount of available data steadily increases \parencite{Alansari2018}. Simultaneously, the possibilities to use the data to draw conclusions increases. This data is generally used to draw conclusions such as failure of a system or a medical issue, such as a heart attack. These events typically occur very rarely \parencite{Hauskrecht2007}. However, when the number of instances of each class is approximately equal, most machine learning algorithms function best. Problems occur when the number of instances of one class greatly exceeds the number of instances of the other. This issue is very popular in practice, and it can be observed in a variety of fields such as fraud detection, medical diagnosis, oil spillage detection, facial recognition, and so on \parencite{Thabtah2020}. The task of identifying the rare item, event or observation is often referred to as anomaly detection. Typically, the anomalous item translates to problems such as bank fraud or medical problems. Often, the anomaly does not adhere to the common statistical definition of an outlier. Therefore, many outlier detection methods (in particular unsupervised methods) fail on such data \parencite{Hodge2004}

A special discipline in anomaly detection is to find the anomaly in a time series. The anomaly detection problem for time series is usually formulated as finding outlier data points relative to some standard or usual signal. Time series anomaly detection plays a critical role in automated monitoring systems. It is an increasingly important topic today, because of its wider application in the context of the Internet of Things (IoT), especially in industrial environments. Popular techniques to find the anomalies are:

\begin{itemize}
	\item Statistical Methods
	\item Support Vector Machines
	\item Clustering 
	\item Density-based Techniques
	\item Neural Networks 
\end{itemize}

Currently neural networks are regarded the cutting-edge research, although first approaches to use artificial neural networks exist since 1958 \parencite{Rosenblatt1958}. They are popular in research for only about 15 to 20 years. Neural networks are well-suited for assisting people in solving complex problems in real-world situations. They can learn and model nonlinear and complex relationships between inputs and outputs; make generalizations and inferences; uncover hidden relationships, patterns, and predictions; and model highly volatile data (such as financial time series data) and variances required to predict rare events (such as fraud detection). Since neural networks do not require time intensive feature engineering, but instead learn the features themselves a lot of time can be saved setting up a model compared to traditional machine learning approaches. At the moment, there is a lot of research focussing on neural networks, which is why scientist generally expect further improvements on this kind of technology (e.g. \parencite{Braei2020}, \parencite{Thabtah2020}, \parencite{Verner2019}, \parencite{Wen2019}). This promising outlook is why this research paper also purely focusses on Neural Networks for anomaly detection. Convolutional Neural Networks and Recurrent Neural Networks are two neural network topologies that are commonly employed. Convolutional Neural Networks are commonly utilized for image identification tasks, whereas Recurrent Neural Networks are used for time series prediction. Recently, a method for detecting anomalies in time series data with Convolutional Neural Networks was proposed \parencite{Wen2019}. Therefore, especially Recurrent Neural Networks and Convolutional Neural Networks are investigated and compared.


\section{Definitions}
Following, important terms in the context of anomaly detection using neural networks, for a better understanding of the problem statement, are elaborated and defined. 

\subsection{Univariate and Multivariate Data}
Time series data investigation poses a special discipline. Generally, anomalies in time series are harder to detect for traditional statistical models, since the possibility of long term dependencies exist. Time series data comes in different forms. It is distinguished between univariate, bivariate and multivariate data \parencite{pena2011}. Univariate involves the analysis of a single variable while bivariate and multivariate analysis examines two or more variables. One significant difference between time-series and other datasets is that the observations are dependent not only on the components, but also on the time features \parencite{Tsay2000}. Thus, time-series analysis and the statistical methods employed are largely distinct from methods employed for random variables, which assume independence and constant variance of the random variables. Time-series are important to data analysts in a variety of fields such as the economy, healthcare and medical research, trading, engineering, and geophysics . These data are used for forecasting and detecting anomalies.

\subsubsection{Univariate Data}
There is only one variable in this type of data. Because the information only deals with one variable that changes, univariate data analysis is the simplest type of analysis. It is not concerned with causes or relationships, and the primary goal of the analysis is to describe the data and identify patterns \parencite{pena2011}.

%\subsubsection{Bivariate Data}
%This type of data involves two different variables. This type of data analysis is concerned with causes and relationships, and the goal is to determine the relationship between the two variables.

\subsubsection{Multivariate Data}
Multivariate data is defined as data that contains three or more variables \parencite{Olkin2001}. It's similar to bivariate, but there are more dependent variables meaning there is not only one variable that influences the observed behaviour (independent variable) but several. The methods for analysing this data are determined by the objectives to be met. Regression analysis, path analysis, factor analysis, and multivariate analysis of variance are some of the techniques. Data collected from a weather station, where temperature, wind speed, dew point are recorded is an example of a multivariate time-series \parencite{Singh2018}.

\subsection{Neural Networks}

An Artificial Neural Network (ANN) with several layers between the input and output layers, is known as a Deep Neural Network (DNN). Neural networks come in a variety of shapes and sizes, but they all have the same basic components: neurons, weights and functions \parencite{Fumo2017}. These components work in a similar way as human brains and can be trained just like any other machine learning algorithm.

\subsubsection{Neuron}
Artificial neurons represent the smallest building blocks of neural networks. A neuron usually receives separately weighted inputs which it sums. The sum is then passed through the activation function to calculate the output of the neuron \parencite{Fumo2017}. When training a neural network, the input weights are adjusted by the optimizer function (see section \ref{optimizer}) to improve accuracy of the given task e.g. classification.

\subsubsection{Layer}
Often neural networks use multiple layers. These layers are then stacked on top of each other, creating a so-called deep architecture, which is why, when training neural networks it is often referred to as deep learning. In neural networks three different kind of layers are distinguished. There are input, output and hidden layers \parencite{Fumo2017}. A layer can be described as a collection of neurons. All layers between the input and output layer are called hidden layers. In the input layer data is fed into the neural network. The output of the hidden layer is calculated by taking the weighted sums of input and passing it through the activation function. Typically, a more complex problem requires more hidden layers to accurately calculate the output \parencite{Gad2018}. In the output layer the final result e.g. a classification is produced \parencite{Fumo2017}. Figure \ref{fig:layers} shows how a fully connected Neural Network with just one hidden layer could look like.


\begin{figure}[h]
	\centering
	\includegraphics[scale=0.3]{Figures/layers}
	\decoRule
	\caption[Input, Hidden and Output Layers]{Input, Hidden and Output Layers \parencite{DennyBritz2015}}
	\label{fig:layers}
	%https://blog.csdn.net/mydear_11000/article/details/51087980
\end{figure}



\subsubsection{Optimizers} \label{optimizer}
Machine learning, and especially deep learning, has the idea of loss, that tells us how good the model is performing at the moment. The loss function is used to train the network, so it performs better. Essentially, a way to decrease the loss needs to be found, because a lower loss indicates that the model is doing better. In mathematics, the process of minimizing an expression is called optimization. Optimizers are techniques or approaches that are used to adjust the characteristics of the neural network, such as weights, in order to minimise losses \parencite{Doshi2019}.

\subsubsection{Supervised vs. Unsupervised Learning}
Supervised learning refers to a task, where a machine learning algorithm learns on data that is labelled \parencite{Russell2010}. A time series that contains an anomaly that is labelled as such would be an example of such a data set. In contrast, when a machine learning algorithm is applied to an unlabelled data set, it is called unsupervised learning \parencite{Hinton1999}. The goal of unsupervised learning is to find hidden patterns (clusters) in the data set.


\section{Background} \label{background}

Following, it is explained how different kinds of neural networks work and what they are used for.

\subsection{Neural Networks for Anomaly Detection} \label{NN}

Out of the three most popular neural network architectures, convolutional neural networks (CNN), recurrent neural networks (RNN) and deep neural networks (DNN), RNNs are typically used for anomaly detection in time series [e.g \parencite{Malhotra2015}, \parencite{Verner2019}]. RNNs have built-in memory and are therefore able to anticipate the next value in a time series based on current and past data. Classic or vanilla RNNs can theoretically keep track of arbitrary long-term dependencies in input sequences. There, however, is a computational issue: when using backpropagation to train a vanilla RNN, the back-propagated gradients can "vanish" or "explode" due to the computations involved in the process, which use finite-precision numbers. Because Long Short Term Memory (LSTM) units allow gradients to flow unchanged, RNNs using LSTM unit or Gated Recurrent units (GRU) partially solve the vanishing gradient problem and therefore drastically improve accuracy \parencite{Hochreiter1997}.

Specially to mention in this context are LSTM (Long-Short Term Memory) and GRU (Gated Recurrent Units). Both achieved outstanding performance when used for tasks such as unsegmented, connected handwriting recognition, speech recognition and anomaly detection in network traffic or IDSs (intrusion detection systems) \parencite{JunyoungChung2014}

\subsubsection{LSTM} \label{LSTM}
LSTM was first proposed in 1997 by Schmidhuber and Hochreiter \parencite{Hochreiter1997}. The initial version to the LSTM unit consisted of a cells, input and output gates. In 1999, the LSTM architecture was improved by introducing a forget gate and therefore allowing the LSTM to reset its own state \parencite{Gers2000}. LSTM is used in a supervised training approach, that means it tries to predict a predefined state taking the past and the current state. If the predicted state differs from the expected state, the weights of the different gates are adjusted using an optimizer algorithm such as gradient descent. Figure \ref{fig:LSTM} shows how the gates and the cell are arranged. The cell represents the memory of the LSTM. In simple words, the LSTM works as follows to predict a new value: 

\begin{enumerate}
	\item Forget Gate: Obsolete information is removed from the cell state.
	\item Input Gate: New information is added to the cell state
	\item Output Gate: The new information and the cell state are added to make the new prediction.
	\item The new cell state is propagated to the next LSTM unit  
\end{enumerate}
     
\begin{figure}[h]
	\centering
	\includegraphics[scale=0.5]{Figures/LSTM}
	\decoRule
	\caption[Gates and Cell of LSTM]{Gates and Cell of LSTM  \parencite{MichaelPhi2018}}
	\label{fig:LSTM}
\end{figure}

% from https://towardsdatascience.com/illustrated-guide-to-lstms-and-gru-s-a-step-by-step-explanation-44e9eb85bf21

%\subsubsection{GRU}
% maybe not necessary

\subsubsection{CNN} \label{CNN}
In contrast to RNNs Convolutional Neural Networks are generally used for image classification. CNNs work as feature extractors and are able to recognize patterns. CNNs use layers that are not fully connected, to reduce complexity (compare to \ref{NN}). In a CNN, a set number of neurons forms a filter \parencite{LeCun1998}. These filters or kernels are the actual feature extractors. A filter may represent a line or pattern (see Figure \ref{fig:filter}) \parencite{RichStureborg2019}.

\begin{figure}[h]
	\centering
	\includegraphics[scale=0.6]{Figures/filter}
	\decoRule
	\caption[Example of a Filter used in CNN]{Example of a Filter used in CNN \parencite{RichStureborg2019}}
	\label{fig:filter}
\end{figure}

To detect whether, a feature is occurrent in a picture, the filter is gradually moved over the picture in so called strides. In every step (stride) the dot-product between the filter and the part of the picture is calculated. The results of the operations are stored in activation maps.  The greater the dot-product the more alike are the filter and the section of the image. Training the network hereby refers to determining the shapes of these filters \parencite{RichStureborg2019}.

Other typical features of a CNN are the pooling layers. The pooling layers reduce the amount of computation necessary. The most commonly used pooling technique is max-pooling and works as shown in Figure \ref{fig:pooling}.

\begin{figure}[h]
	\centering
	\includegraphics[scale=0.2]{Figures/pooling}
	\decoRule
	\caption[Example of max-pooling]{Example of max-pooling \parencite{RichStureborg2019}}
	\label{fig:pooling}
\end{figure}

The idea of max-pooling is to only keep the maximum value of an activation map, In the orange region 7 represents the maximum value, so it is kept while the other values are discarded \parencite{RichStureborg2019}. Using this technique, the sharpest features are extracted.

In 2019, Wen and Keys proposed to use CNN also for anomaly detection in time series since it shares many common aspects with image segmentation. A univariate time series is therefore viewed as a one-dimensional image \parencite{Wen2019}.\\ 


% https://towardsdatascience.com/conv-nets-for-dummies-a-bottom-up-approach-c1b754fb14d6


%\subsection{Transfer Learning}
%%https://builtin.com/data-science/transfer-learning
%%Section about transfer learning\\
%The reuse of a previously trained model on a new problem is known as transfer learning. It is currently very popular in deep learning because it can train deep neural networks with a relatively small amount of data. This is particularly useful in the field of data science, as most real-world problems do not provide millions of labelled data points to train complex models. Further, transfer learning can save time, since the a particular does not need be trained from scratch.
%In transfer learning, the knowledge of an already trained machine learning model is applied to a different but related problem. For example, if a classifier was trained to predict if an image contains a backpack, the model's experience could be applied to recognize other objects such as sunglasses \parencite{NiklasDonges2020}.

\section{Problem Statement} \label{Problem}

Defining a ground truth is a difficult aspects of time series anomaly detection. Determining when anomalous behaviour begins and ends in time series is a difficult task, as even human experts are likely to disagree in their assessments. Furthermore, there is the question of what constitutes a useful detection when detecting anomalies in time series.
In the past, RNN have successfully been used for anomaly detection (e.g. [\parencite{Malhotra2015}; \parencite{Fan2016}]. Therefore, designs for various use cases are well researched. RNN are well suited for the task, however, they take a long time to train due to the complexity of how a single unit is designed (see \ref{LSTM}). In comparison CNN are not as complex and therefore, generally take less time to train. However, CNNs are generally used for image recognition and were only very recently used for anomaly detection in time series. It is therefore mostly unknown what designs are applicable for successful anomaly detection in time series data. %is this true?
While RNN are able to deal with multivariate data by design, a classical CNN requires design changes to be able to deal with multivariate data. Wen and Keys \parencite*{Wen2019} proposed to use a special kind of U-net, an improved version of a Fully Convolutional Neural Network \parencite{Ronneberger2015}.

Further, a CNN is not capable to analyse streaming data, so it relies on segmentation of the data. These data segments are called snapshots \parencite{Wen2019}. In order to not miss any data points, the frequency of taking these snapshots should be at least as high as the length of snapshot so that every time point is evaluated by the model at least once. However, for better performance it might prove beneficial to use a higher frequency which means every point is evaluated various times by the model \parencite{Wen2019}. The proposed design change and the fact that every point is evaluated multiple times, increases complexity and evaluation time and therefore counteract the architectural advantage of CNN compared to an RNN. 

When designing a neural network many parameters must be chosen, this applies to both mentioned types of Neural Networks. For example, when designing a CNN, the number of layers, the activation function(s) of a single neuron and the optimizer function have to be chosen. Similarly, when designing an RNN also the number of layers and the optimizer function must be determined. Because the basic building blocks of both networks types are very different it is difficult to fairly compare the complexity of two architecture approaches. Another important parameter which applies to both network types is the number of epochs for which the networks are trained. Through the epochs the training time is determined. In order to compare the two types of neural networks, two networks of similar complexity have to be designed. With equal training time the performance of both could be compared and evaluated. An RNN is therefore only set up as benchmark while the main goal of this research project is to clarify whether CNNs are really useful and propose an advantage over RNNs when applied on time series data for anomaly detection.


\section{Thesis Statement} \label{thesisstatement}

Convolutional Neural Networks are superior to Recurrent Neural Networks when looking for anomalies in time series data regarding overall performance, training and inference time, and complexity.

\subsection{Research Questions} \label{research_questions}

\begin{enumerate}
	\item How does a CNN for univariate and multivariate data need to be designed for successful anomaly detection in time series data?
	\item What advantages and disadvantages arise when using a CNN compared to a RNN for anomaly detection in univariate and multivariate time series?
	\item Which parameter settings are crucial for a fair performance comparison between RNN and CNN? 
\end{enumerate}

These research questions should aid in the formulation of a meaningful claim concerning the thesis statement. To validate the thesis statement, the first question should demonstrate that constructing a CNN for anomaly detection is not more difficult than designing an RNN.
Furthermore, studying how the CNN should be constructed lays the groundwork for the experiments to follow. The second research question should demonstrate the benefits and drawbacks that arise when creating but also when using CNN to univariate and multivariate data. To back up the thesis statement, there should be no substantial drawbacks to using a CNN for anomaly detection. The last question provides answers on how hyper-parameters should be established for a fair comparison in order to make a credible judgment on the thesis statement.  The answer to the question makes sure that the comparison is fair and that neither of the neural networks is favored by the design of experiments. 


\clearpage 
\subsection{Research Objectives}

Following the research objectives of this paper are defined.

%always start with a verb ... to test, to determine

\begin{enumerate}
	\item Determine what design changes a CNN requires to detect anomalies in time series data.
	\item Determine how the CNN should be designed for the comparison with a RNN
	\item State the advantages and disadvantages of the chosen CNN architecture.
	\item Define parameters which allow a fair comparison of CNN and RNN
\end{enumerate}

\subsection{Limitations}

Recently there have been approaches that combine CNN and RNN into a hybrid network for tasks such as handwriting recognition or video-based emotion recognition \parencite{Dutta2018} \parencite{Fan2016}. However, this paper only compares pure CNN and RNN, and does not investigate a hybrid approach.

% AUC and ROC not explained
% Test and training not explained 

\subsection{Significance}

Until now, time series data, and especially anomaly detection on time series data, was foremost approached with RNNs. This paper should answer the question whether CNNs propose a valid alternative and even propose some advantages over RNNs. The paper will answer the fundamental question whether research should channel efforts to further investigate CNNs for anomaly detection in time series data or whether no benefits can be discovered and research is better to focus on other areas. 

%----------------------------------------------------------------------------------------
