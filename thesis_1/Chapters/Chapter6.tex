\chapter{Results Analysis}

In this chapter, the results of all experiments are compared and discussed to evaluate which approach is suitable in what scenario. 

The experiments quickly showed that when trying to detect anomalies in time series with deep learning approaches, the decribed unsupervised approach is the preferred choice. As supervised approaches need to assure, that the anomalies that occur when the model is in operation are the same as in the training set. Further, a training set that contains enough anomalies must exist. Only under these limiting conditions a supervised model can reliably be deployed. When these requirements are fullfilled a Convolutional Neural Network might be a good choice. 
Experiment 1 showed that a CNN, in comparison to a RNN, is capable of recognizing the anomalies. The same conclusion was reached by Wen and Keyes \parencite*{Wen2019}, who successfully applied a special version of a CNN (U-Net: Section \ref{U-Net}) to the GHL dataset.  

In contrast to the supervised approach the unsupervised approach generally proved to be more stable. As it needs no knowledge of the nature of the anomalies. The only requirement for this approach is a sufficiently large training set to learn the normal behaviour. Both neural network types, CNN and RNN, showed similar abilities when learning the normal behaviour. This means that CNN, that are usually applied for tasks such image recognition, are similarly suitable to predict time series, which is normally the domain of RNN. Looking at the F1-Scores of Experiments 1 to 3, shows that CNN can be useful alternative to RNN when detecting anomalies. Although, the F1-Score of the CNN is often marginally lower than the F1-Score of the compared RNN, the CNN has another big advantage. When tracking anomalies, it is often a neccessary to detect them as quickly as possible. Looking at the inference times of the various models, it can be seen that the CNN model is up to three times faster than the RNN model. This advantage of speed can be of paramount importance in a real world scenario, as it can decide if delicate components of a system take damage or not.

The CNN is not only faster at inference but also in training. As there many hyper-parameters, such as batch size, lookback, layers etc., to be determined for an optimal model performance, the training time becomes important. When using a machine learning algorithm, a model is trained again and again to find the best configuration. So the training time of a model decides how fast a well performing model can be developed. Looking at the training times achieved in this work, it can be seen that the CNN learns much faster. This might have two reasons. First, when the number of parameters in the CNN and RNN models are compared, it can be observed, that a CNN generally has less parameters (see Appendix \ref{AppendixA}). When training a neural network, after every batch, it needs to be calculated how much these parameters need to be adjusted. This makes it more and more computationally expensive the more parameters there are. Second, it seems to be good strategy to look at a time series as a pattern. Where the RNN calculated the influence of the past on the future, the CNN tries to recognise patterns in the data. This approach not only worked when the time series followed a cyclic pattern as Experiment 1 and 3 but also in the case of the weather data in Experiment 2.     

Ultimately, as it is often the case in data science, the choice whether to use RNN or CNN is a tradeoff, in this particular case, between speed and quality of the result. A tradeoff, which might, however, be mitigated, in the favor of the CNN, by using more advanced versions of the CNN such as the U-Net.


\section{Review of Experiments}
Neural Networks became popular in recent years in anomaly detection as they require little domain knowledge. The experiments conducted in this work indicate that this comes at the expense of higher training and inference times as well as performance drawbacks. This is particularly evident in Experiment 3. As Figure \ref{fig:review} shows, one type of anomaly is easily recognizable. The green line representes the actual value, the red line indicates the anomalies. An anomaly occurs when the green line surpasses the 333Â°C threshold represented by the black line. 

\begin{figure}[h]
	\centering
	\includegraphics[scale=0.45]{Figures/review}
	\decoRule
	\caption[Behaviour of Anomaly in the GHL Dataset]{Behaviour of Anomaly in the GHL Dataset \parencite{own}}
	\label{fig:review}
\end{figure}

This circumstance is supposed to be known by someone with domain expertise.
As a result, it is known that for this sort of anomaly, a logistic regression model is sufficient, and on top of that, this much simpler model achieves a 100 percent accuracy. 

To conclude, deep learning is not always the best option, and domain expertise is still necessary to select the quickest and most accurate model.   

%Looking at the computation times and the results of the experiments, it is however, questionable if deep learning is the right approach in each case. In Experiment 1, an algorithm that simply predicts a data point at the same time in the future as equal to now.


% Critic, there might still be better approaches than NNs


%To compare different models often a process called k-fold cross validation is used. When applying k-fold cross-validation, k different validation sets are drawn from the dataset, as opposed to just one when applying the validation set approach, which was used in this work, therefore a model needs to be trained k times. 

\chapter{Conclusion}

This final chapter seeks to answer the research questions proposed in Section \ref{research_questions}. Further, the results of this work are discussed in regard to the thesis statement proposed in Section \ref{thesisstatement}

\section{Answering the Research Questions}


\paragraph{How does a CNN for univariate and multivariate data need to be designed for successful anomaly detection in time series data?}

Munir \parencite*{Munir2019} showed that for univariate time series data 1-dimensional convolutional layers can successfully be used. Wen and Keyes \parencite*{Wen2019} also use 1-dimensional convolutional layers for their proposed architecture type, the U-Net. The further design of the CNN for anomaly detection is dependent on the given task. If a supervised approach is deemed suitable, the CNN is defined as a classifier deciding whether a given sequence is anomalous or not. However, this approach is reserved only for a few special use cases where the shapes of the anomalies are known and unchanging. If it is unknown how the anomalies look like, the CNN has to be designed to predict the future of a time series. Given this so-called regression task, it is preferable to omit the max-pooling layers, that are normally part of CNN architecture, because especially with complex multivariate time series, they impair a CNN's ability to reliably predict a time series.  

\paragraph{What advantages and disadvantages arise when using a CNN compared to a RNN for anomaly detection in univariate and multivariate time series?} The main advantage of a CNN over an RNN, such as a GRU- or LSTM-neural network, is its shorter computation time. The research revealed that utilizing a CNN reduces the amount of time it takes to train and infer. This is especially useful in the use case of anomaly detection as it is crucial to recognize the anomalies as fast as possible.

The second big advantage of a CNN is its ability to recognize anomalies. Given a case where the anomalous behaviour always follows the same pattern as in Experiment 1, a CNN can reliably be trained to recognize this pattern. A real world scenario for this use case be the frequency analysis of a ball bearing \parencite{Mais2002}.

In the scope of this work no major disadvantages were found when comparing CNN to RNN to detect anomalies.


\paragraph{What hyper-parameter settings are crucial for a fair performance comparison between RNN and CNN?} In order to fairly compare the two neural network architectures, whereever possible the hyperparameters were defined for both types. These hyperparameter include foremost the configuration of the generator function. Further, regarding the neural network architecture, the given task and the applied optimizer was defined the same for both types. At last, when postprocessing the data, that was produced by the experiments, parameters were set to the same value, for example how the rolling mean was calculated in Experiment 3.

In contrast, it is difficult to compare the neural network architectures, because the functionalities are fundamentally different. Even when the neural networks have the same number of layers and roughly the same number of neurons, as in Experiment 2, the number of trainable parameters is vastly different (see Appendix \ref{exp2_summary}). During the experiments conducted in this work, the hyperparameters defining the neural networks were set on a experimental basis. No hyperparameter-tuning routine was used, with the disadvantage of probably suboptimal results, but with the advantage of being able to build neural networks with similar complexity. Except for Experiment 3, the neural networks were designed with the same number of layers and approximately the same number of neurons. Interestingly, this approach showed, that the learning capabilities of the neural networks were similar.

\section{Comment on Thesis Statement}

The experiments conducted in this work did not show that Convolutional Neural Networks are overall superior to Recurrent Neural Networks. They have, nevertheless, been proved to offer significant benefits, particularly in the context of anomaly detection. The biggest advantage of CNN, is the reduced computation time, which helps to faster identify anomalies. The shorter computation time further becomes important once the experiment is scaled to bigger datasets and faster GPU aided hardware. At a certain complexity, the cost to run such deep learning based models will become incisive. Since deep learning is also sometimes critised to not be eco-friendly, CNN offer an approach to mitigate both of these problems \parencite{Walleser2021}.

The trials conducted in this paper revealed that when a typical CNN with convolutional and max-pooling layers is applied, the performance is still inferior to that of an RNN. This gap, however migth be closed, with the use of upsampling and transposing layers. These, also called deconvolutional layers, build the counterpart to convolutional layers. Simply said convolutional layers' goal is to extract features, whereas deconvolutional layers' goal is to produce features. Wen and Keyes \parencite*{Wen2019} were able to successfully use the combination of both, convolution and deconvolution, in their anomaly detection experiment.

Overall, the experiment demonstrated that CNNs are capable of accuratly forecasting time series. With some adjustments, they are probaly equally as accurate as RNN. This work supports the view of some experts who believe RNN will be phased out in the future \parencite{Culurciello2018} \parencite{Bai2018}. 

% Some argue that RNN are no longer needed https://www.macnica.co.jp/business/ai_iot/columns/135112/
%https://towardsdatascience.com/the-fall-of-rnn-lstm-2d1594c74ce0
%https://arxiv.org/abs/1803.01271
\section{Future Research}




