@misc{Google2020,
author = {Google},
booktitle = {Machine Learning Crash Course},
title = {{Classification: ROC Curve and AUC}},
url = {https://developers.google.com/machine-learning/crash-course/classification/roc-and-auc},
year = {2020}
}

@phdthesis{Verner2019,
author = {Verner, Alexander},
pages = {1--131},
school = {Nova Southeastern University},
title = {{No Title}},
url = {https://nsuworks.nova.edu/gscis_etd/1074.},
year = {2019}
}
@misc{Gupta2018,
abstract = {Deep neural networks have shown promising results for various clinical prediction tasks such as diagnosis, mortality prediction, predicting duration of stay in hospital, etc. However, training deep networks - such as those based on Recurrent Neural Networks (RNNs) - requires large labeled data, high computational resources, and significant hyperparameter tuning effort. In this work, we investigate as to what extent can transfer learning address these issues when using deep RNNs to model multivariate clinical time series. We consider transferring the knowledge captured in an RNN trained on several source tasks simultaneously using a large labeled dataset to build the model for a target task with limited labeled data. An RNN pre-trained on several tasks provides generic features, which are then used to build simpler linear models for new target tasks without training task-specific RNNs. For evaluation, we train a deep RNN to identify several patient phenotypes on time series from MIMIC-III database, and then use the features extracted using that RNN to build classifiers for identifying previously unseen phenotypes, and also for a seemingly unrelated task of in-hospital mortality. We demonstrate that (i) models trained on features extracted using pre-trained RNN outperform or, in the worst case, perform as well as task-specific RNNs; (ii) the models using features from pre-trained models are more robust to the size of labeled data than task-specific RNNs; and (iii) features extracted using pre-trained RNN are generic enough and perform better than typical statistical hand-crafted features.},
author = {Gupta, Priyanka and Malhotra, Pankaj and Vig, Lovekesh and Shroff, Gautam},
booktitle = {arXiv},
issn = {23318422},
title = {{Transfer learning for clinical time series analysis using recurrent neural networks}},
year = {2018}
}

@book{Chollet2018,
author = {Chollet, Fran{\c{c}}ois and Allaire, Joseph J.},
isbn = {9781617295546},
pages = {1--360},
publisher = {Manning Publications},
title = {{Deep Learning with R}},
year = {2018}
}
@article{LeCun1998,
abstract = {Multilayer neural networks trained with the back-propagation algorithm constitute the best example of a successful gradient-based learning technique. Given an appropriate network architecture, gradient-based learning algorithms can be used to synthesize a complex decision surface that can classify high-dimensional patterns, such as handwritten characters, with minimal preprocessing. This paper reviews various methods applied to handwritten character recognition and compares them on a standard handwritten digit recognition task. Convolutional neural networks, which are specifically designed to deal with the variability of two dimensional (2-D) shapes, are shown to outperform all other techniques. Real-life document recognition systems are composed of multiple modules including field extraction, segmentation, recognition, and language modeling. A new learning paradigm, called graph transformer networks (GTN's), allows such multimodule systems to be trained globally using gradient-based methods so as to minimize an overall performance measure. Two systems for online handwriting recognition are described. Experiments demonstrate the advantage of global training, and the flexibility of graph transformer networks. A graph transformer network for reading a bank check is also described. It uses convolutional neural network character recognizers combined with global training techniques to provide record accuracy on business and personal checks. It is deployed commercially and reads several million checks per day. {\textcopyright} 1998 IEEE.},
author = {LeCun, Yann and Bottou, L{\'{e}}on and Bengio, Yoshua and Haffner, Patrick},
doi = {10.1109/5.726791},
issn = {00189219},
journal = {Proceedings of the IEEE},
keywords = {Convolutional neural networks,Document recognition,Finite state transducers,Gradient-based learning,Graph transformer networks,Machine learning,Neural networks,Optical character recognition (OCR)},
number = {11},
pages = {2278--2323},
title = {{Gradient-based learning applied to document recognition}},
volume = {86},
year = {1998}
}
@article{Munir2019,
abstract = {Traditional distance and density-based anomaly detection techniques are unable to detect periodic and seasonality related point anomalies which occur commonly in streaming data, leaving a big gap in time series anomaly detection in the current era of the IoT. To address this problem, we present a novel deep learning-based anomaly detection approach (DeepAnT) for time series data, which is equally applicable to the non-streaming cases. DeepAnT is capable of detecting a wide range of anomalies, i.e., point anomalies, contextual anomalies, and discords in time series data. In contrast to the anomaly detection methods where anomalies are learned, DeepAnT uses unlabeled data to capture and learn the data distribution that is used to forecast the normal behavior of a time series. DeepAnT consists of two modules: time series predictor and anomaly detector. The time series predictor module uses deep convolutional neural network (CNN) to predict the next time stamp on the defined horizon. This module takes a window of time series (used as a context) and attempts to predict the next time stamp. The predicted value is then passed to the anomaly detector module, which is responsible for tagging the corresponding time stamp as normal or abnormal. DeepAnT can be trained even without removing the anomalies from the given data set. Generally, in deep learning-based approaches, a lot of data are required to train a model. Whereas in DeepAnT, a model can be trained on relatively small data set while achieving good generalization capabilities due to the effective parameter sharing of the CNN. As the anomaly detection in DeepAnT is unsupervised, it does not rely on anomaly labels at the time of model generation. Therefore, this approach can be directly applied to real-life scenarios where it is practically impossible to label a big stream of data coming from heterogeneous sensors comprising of both normal as well as anomalous points. We have performed a detailed evaluation of 15 algorithms on 10 anomaly detection benchmarks, which contain a total of 433 real and synthetic time series. Experiments show that DeepAnT outperforms the state-of-the-art anomaly detection methods in most of the cases, while performing on par with others.},
author = {Munir, Mohsin and Siddiqui, Shoaib Ahmed and Dengel, Andreas and Ahmed, Sheraz},
doi = {10.1109/ACCESS.2018.2886457},
issn = {21693536},
journal = {IEEE Access},
keywords = {Anomaly detection,artificial intelligence,convolutional neural network,deep neural networks,recurrent neural networks,time series analysis},
pages = {1991--2005},
title = {{DeepAnT: A Deep Learning Approach for Unsupervised Anomaly Detection in Time Series}},
volume = {7},
year = {2019}
}
@inproceedings{Cicek2016,
abstract = {This paper introduces a network for volumetric segmentation that learns from sparsely annotated volumetric images. We outline two attractive use cases of this method: (1) In a semi-automated setup,the user annotates some slices in the volume to be segmented. The network learns from these sparse annotations and provides a dense 3D segmentation. (2) In a fully-automated setup,we assume that a representative,sparsely annotated training set exists. Trained on this data set,the network densely segments new volumetric images. The proposed network extends the previous u-net architecture from Ronneberger et al. by replacing all 2D operations with their 3D counterparts. The implementation performs on-the-fly elastic deformations for efficient data augmentation during training. It is trained end-to-end from scratch,i.e.,no pre-trained network is required. We test the performance of the proposed method on a complex,highly variable 3D structure,the Xenopus kidney,and achieve good results for both use cases.},
author = {{\c{C}}i{\c{c}}ek, {\"{O}}zg{\"{u}}n and Abdulkadir, Ahmed and Lienkamp, Soeren S. and Brox, Thomas and Ronneberger, Olaf},
booktitle = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
doi = {10.1007/978-3-319-46723-8_49},
issn = {16113349},
title = {{3D U-net: Learning dense volumetric segmentation from sparse annotation}},
volume = {9901 LNCS},
year = {2016}
}
@inproceedings{Zheng2014,
abstract = {Time series (particularly multivariate) classification has drawn a lot of attention in the literature because of its broad applications for different domains, such as health informatics and bioinformatics. Thus, many algorithms have been developed for this task. Among them, nearest neighbor classification (particularly 1-NN) combined with Dynamic Time Warping (DTW) achieves the state of the art performance. However, when data set grows larger, the time consumption of 1-NN with DTW grows linearly. Compared to 1-NN with DTW, the traditional feature-based classification methods are usually more efficient but less effective since their performance is usually dependent on the quality of hand-crafted features. To that end, in this paper, we explore the feature learning techniques to improve the performance of traditional feature-based approaches. Specifically, we propose a novel deep learning framework for multivariate time series classification. We conduct two groups of experiments on real-world data sets from different application domains. The final results show that our model is not only more efficient than the state of the art but also competitive in accuracy. It also demonstrates that feature learning is worth to investigate for time series classification. {\textcopyright} 2014 Springer International Publishing Switzerland.},
author = {Zheng, Yi and Liu, Qi and Chen, Enhong and Ge, Yong and Zhao, J. Leon},
booktitle = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
doi = {10.1007/978-3-319-08010-9_33},
isbn = {9783319080093},
issn = {16113349},
pages = {298--310},
title = {{Time series classification using multi-channels deep convolutional neural networks}},
volume = {8485 LNCS},
year = {2014}
}
@inproceedings{Chauhan2015,
abstract = {Electrocardiography (ECG) signals are widely used to gauge the health of the human heart, and the resulting time series signal is often analyzed manually by a medical professional to detect any arrhythmia that the patient may have suffered. Much work has been done to automate the process of analyzing ECG signals, but most of the research involves extensive preprocessing of the ECG data to derive vectorized features and subsequently designing a classifier to discriminate between healthy ECG signals and those indicative of an Arrhythmia. This approach requires knowledge and data of the different types of Arrhythmia for training. However, the heart is a complex organ and there are many different and new types of Arrhythmia that can occur which were not part of the original training set. Thus, it may be more prudent to adopt an anomaly detection approach towards analyzing ECG signals. In this paper, we utilize a deep recurrent neural network architecture with Long Short Term Memory (LSTM) units to develop a predictive model for healthy ECG signals. We further utilize the probability distribution of the prediction errors from these recurrent models to indicate normal or abnormal behavior. An added advantage of using LSTM networks is that the ECG signal can be directly fed into the network without any elaborate preprocessing as required by other techniques. Also, no prior information about abnormal signals is needed by the networks as they were trained only on normal data. We have used the MIT-BIH Arrhythmia Database to obtain ECG time series data for both normal periods and for periods during four different types of Arrhythmias, namely Premature Ventricular Contraction (PVC), Atrial Premature Contraction (APC), Paced Beats (PB) and Ventricular Couplet (VC). Results are promising and indicate that Deep LSTM models may be viable for detecting anomalies in ECG signals.},
author = {Chauhan, Sucheta and Vig, Lovekesh},
booktitle = {Proceedings of the 2015 IEEE International Conference on Data Science and Advanced Analytics, DSAA 2015},
doi = {10.1109/DSAA.2015.7344872},
isbn = {9781467382731},
title = {{Anomaly detection in ECG time signals via deep long short-term memory networks}},
year = {2015}
}
@inproceedings{Malhotra2015,
abstract = {Long Short Term Memory (LSTM) networks have been demonstrated to be particularly useful for learning sequences containing longer term patterns of unknown length, due to their ability to maintain long term memory. Stacking recurrent hidden layers in such networks also enables the learning of higher level temporal features, for faster learning with sparser representations. In this paper, we use stacked LSTM networks for anomaly/fault detection in time series. A network is trained on non-anomalous data and used as a predictor over a number of time steps. The resulting prediction errors are modeled as a multivariate Gaussian distribution, which is used to assess the likelihood of anomalous behavior. The efficacy of this approach is demonstrated on four datasets: ECG, space shuttle, power demand, and multi-sensor engine dataset.},
author = {Malhotra, Pankaj and Vig, Lovekesh and Shroff, Gautam and Agarwal, Puneet},
booktitle = {23rd European Symposium on Artificial Neural Networks, Computational Intelligence and Machine Learning, ESANN 2015 - Proceedings},
isbn = {9782875870148},
pages = {89--94},
title = {{Long Short Term Memory networks for anomaly detection in time series}},
year = {2015}
}
@article{Chandola2009,
author = {Chandola, Varun and Banerjee, Arindam and Kumar, Vipin},
journal = {ACM computing surveys (CSUR)},
number = {3},
pages = {1--58},
title = {{Anomaly detection: A survey}},
volume = {41},
year = {2009}
}
@article{Ord1996,
abstract = {No abstract is available for this item.},
author = {Ord, Keith},
journal = {International Journal of Forecasting},
number = {1},
title = {{Outliers in statistical data : V. Barnett and T. Lewis, 1994, 3rd edition, (John Wiley and Sons, Chichester), 584 pp., [UK pound]55.00, ISBN 0-471-93094-6}},
volume = {12},
year = {1996}
}
@book{Aggarwal2013,
abstract = {With the increasing advances in hardware technology for data collection, and advances in software technology (databases) for data organization, computer scientists have increasingly participated in the latest advancements of the outlier analysis field. Computer scientists, specifically, approach this field based on their practical experiences in managing large amounts of data, and with far fewer assumptions- the data can be of any type, structured or unstructured, and may be extremely large. Outlier Analysis is a comprehensive exposition, as understood by data mining experts, statisticians and computer scientists. The book has been organized carefully, and emphasis was placed on simplifying the content, so that students and practitioners can also benefit. Chapters will typically cover one of three areas: methods and techniques commonly used in outlier analysis, such as linear methods, proximity-based methods, subspace methods, and supervised methods; data domains, such as, text, categorical, mixed-attribute, time-series, streaming, discrete sequence, spatial and network data; and key applications of these methods as applied to diverse domains such as credit card fraud detection, intrusion detection, medical diagnosis, earth science, web log analytics, and social network analysis are covered.},
author = {Aggarwal, Charu C.},
booktitle = {Outlier Analysis},
doi = {10.1007/978-1-4614-6396-2},
isbn = {9781461463962},
pages = {1--446},
title = {{Outlier analysis}},
volume = {9781461463},
year = {2013}
}
@misc{Braei2020,
abstract = {Anomaly detection for time-series data has been an important research field for a long time. Seminal work on anomaly detection methods has been focussing on statistical approaches. In recent years an increasing number of machine learning algorithms have been developed to detect anomalies on time-series. Subsequently, researchers tried to improve these techniques using (deep) neural networks. In the light of the increasing number of anomaly detection methods, the body of research lacks a broad comparative evaluation of statistical, machine learning and deep learning methods. This paper studies 20 univariate anomaly detection methods from the all three categories. The evaluation is conducted on publicly available datasets, which serve as benchmarks for time-series anomaly detection. By analyzing the accuracy of each method as well as the computation time of the algorithms, we provide a thorough insight about the performance of these anomaly detection approaches, alongside some general notion of which method is suited for a certain type of data.},
archivePrefix = {arXiv},
arxivId = {2004.00433},
author = {Braei, Mohammad and Wagner, Sebastian},
booktitle = {arXiv},
eprint = {2004.00433},
issn = {23318422},
title = {{Anomaly detection in univariate time-series: A survey on the state-of-the-art}},
year = {2020}
}
@misc{NiklasDonges2020,
author = {{Niklas Donges}},
booktitle = {Built-In},
month = {sep},
title = {{What is transfer learning? Exploring the popular deep learning approach}},
url = {https://builtin.com/data-science/transfer-learning},
urldate = {2021-04-18},
year = {2020}
}
@misc{RichStureborg2019,
author = {{Rich Stureborg}},
booktitle = {Towards Data Science},
month = {jan},
pages = {--undefined},
title = {{Conv Nets for dummies}},
url = {https://towardsdatascience.com/conv-nets-for-dummies-a-bottom-up-approach-c1b754fb14d6},
urldate = {2021-04-18},
year = {2019}
}
@misc{MichaelPhi2018,
author = {{Michael Phi}},
booktitle = {Towards Data Science},
month = {sep},
title = {{Illustrated Guide to LSTM's and GRU's: A step by step explanation}},
url = {https://towardsdatascience.com/illustrated-guide-to-lstms-and-gru-s-a-step-by-step-explanation-44e9eb85bf21},
urldate = {2021-04-18},
year = {2018}
}
@misc{DennyBritz2015,
author = {{Denny Britz}},
booktitle = {WILDML  Artificial Intelligence, Deep Learning, and NLP},
month = {sep},
title = {{Implementing a Neural Network from Scratch in Python – An Introduction}},
url = {http://www.wildml.com/2015/09/implementing-a-neural-network-from-scratch/},
urldate = {2021-04-18},
year = {2015}
}
@article{Hochreiter1997,
abstract = {Learning to store information over extended time intervals by recurrent backpropagation takes a very long time, mostly because of insufficient, decaying error backflow. We briefly review Hochreiter's (1991) analysis of this problem, then address it by introducing a novel, efficient, gradient based method called long short-term memory (LSTM). Truncating the gradient where this does not do harm, LSTM can learn to bridge minimal time lags in excess of 1000 discrete-time steps by enforcing constant error flow through constant error carousels within special units. Multiplicative gate units learn to open and close access to the constant error flow. LSTM is local in space and time; its computational complexity per time step and weight is O. 1. Our experiments with artificial data involve local, distributed, real-valued, and noisy pattern representations. In comparisons with real-time recurrent learning, back propagation through time, recurrent cascade correlation, Elman nets, and neural sequence chunking, LSTM leads to many more successful runs, and learns much faster. LSTM also solves complex, artificial long-time-lag tasks that have never been solved by previous recurrent network algorithms.},
author = {Hochreiter, Sepp and Schmidhuber, J{\"{u}}rgen},
doi = {10.1162/neco.1997.9.8.1735},
issn = {0899-7667},
journal = {Neural Computation},
month = {nov},
number = {8},
pages = {1735--1780},
title = {{Long Short-Term Memory}},
url = {https://direct.mit.edu/neco/article/9/8/1735-1780/6109},
volume = {9},
year = {1997}
}
@article{JunyoungChung2014,
abstract = {In this paper we compare different types of recurrent units in recurrent neural networks (RNNs). Especially, we focus on more sophisticated units that implement a gating mechanism, such as a long short-term memory (LSTM) unit and a recently proposed gated recurrent unit (GRU). We evaluate these recurrent units on the tasks of polyphonic music modeling and speech signal modeling. Our experiments revealed that these advanced recurrent units are indeed better than more traditional recurrent units such as tanh units. Also, we found GRU to be comparable to LSTM.},
archivePrefix = {arXiv},
arxivId = {1412.3555},
author = {Chung, Junyoung and Gulcehre, Caglar and Cho, KyungHyun and Bengio, Yoshua},
eprint = {1412.3555},
journal = {NIPS 2014 Deep Learning and Representation Learning Workshop},
month = {dec},
title = {{Empirical Evaluation of Gated Recurrent Neural Networks on Sequence Modeling}},
url = {http://arxiv.org/abs/1412.3555},
year = {2014}
}
@inproceedings{Fan2016,
abstract = {In this paper, we present a video-based emotion recognition system submitted to the EmotiW 2016 Challenge. The core module of this system is a hybrid network that combines recurrent neural network (RNN) and 3D convolutional networks (C3D) in a late-fusion fashion. RNN and C3D encode appearance and motion information in different ways. Specifically, RNN takes appearance features extracted by convolutional neural network (CNN) over individual video frames as input and encodes motion later, while C3D models appearance and motion of video simultaneously. Combined with an audio module, our system achieved a recognition accuracy of 59.02 without using any additional emotion-labeled video clips in training set, compared to 53.8 of the winner of EmotiW 2015. Extensive experiments show that combining RNN and C3D together can improve video-based emotion recognition noticeably.},
author = {Fan, Yin and Lu, Xiangju and Li, Dian and Liu, Yuanliu},
booktitle = {ICMI 2016 - Proceedings of the 18th ACM International Conference on Multimodal Interaction},
doi = {10.1145/2993148.2997632},
isbn = {9781450345569},
keywords = {3d convolutional network,Emotion recognition,Long short term memory network,Model fusion,Recurrent neural network},
pages = {445--450},
title = {{Video-Based emotion recognition using CNN-RNN and C3D hybrid networks}},
year = {2016}
}
@inproceedings{Dutta2018,
abstract = {The success of deep learning based models have centered around recent architectures and the availability of large scale annotated data. In this work, we explore these two factors systematically for improving handwritten recognition for scanned off-line document images. We propose a modified CNN-RNN hybrid architecture with a major focus on effective training using: (i) efficient initialization of network using synthetic data for pretraining, (ii) image normalization for slant correction and (iii) domain specific data transformation and distortion for learning important invariances. We perform a detailed ablation study to analyze the contribution of individual modules and present state of art results for the task of unconstrained line and word recognition on popular datasets such as IAM, RIMES and GW.},
author = {Dutta, Kartik and Krishnan, Praveen and Mathew, Minesh and Jawahar, C. V.},
booktitle = {Proceedings of International Conference on Frontiers in Handwriting Recognition, ICFHR},
doi = {10.1109/ICFHR-2018.2018.00023},
isbn = {9781538658758},
issn = {21676453},
keywords = {CNN RNN network,Data augmentation,Handwriting recognition,Image pre-processing},
pages = {80--85},
title = {{Improving CNN-RNN hybrid networks for handwriting recognition}},
volume = {2018-Augus},
year = {2018}
}
@article{Shelhamer2017,
abstract = {Convolutional networks are powerful visual models that yield hierarchies of features. We show that convolutional networks by themselves, trained end-to-end, pixels-to-pixels, improve on the previous best result in semantic segmentation. Our key insight is to build 'fully convolutional' networks that take input of arbitrary size and produce correspondingly-sized output with efficient inference and learning. We define and detail the space of fully convolutional networks, explain their application to spatially dense prediction tasks, and draw connections to prior models. We adapt contemporary classification networks (AlexNet, the VGG net, and GoogLeNet) into fully convolutional networks and transfer their learned representations by fine-tuning to the segmentation task. We then define a skip architecture that combines semantic information from a deep, coarse layer with appearance information from a shallow, fine layer to produce accurate and detailed segmentations. Our fully convolutional networks achieve improved segmentation of PASCAL VOC (30% relative improvement to 67.2% mean IU on 2012), NYUDv2, SIFT Flow, and PASCAL-Context, while inference takes one tenth of a second for a typical image.},
archivePrefix = {arXiv},
arxivId = {1411.4038},
author = {Shelhamer, Evan and Long, Jonathan and Darrell, Trevor},
doi = {10.1109/TPAMI.2016.2572683},
eprint = {1411.4038},
issn = {01628828},
journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
keywords = {Convolutional Networks,Deep Learning,Semantic Segmentation,Transfer Learning},
number = {4},
pages = {640--651},
pmid = {27244717},
title = {{Fully Convolutional Networks for Semantic Segmentation}},
volume = {39},
year = {2017}
}
@inproceedings{Ronneberger2015,
abstract = {There is large consent that successful training of deep networks requires many thousand annotated training samples. In this paper, we present a network and training strategy that relies on the strong use of data augmentation to use the available annotated samples more efficiently. The architecture consists of a contracting path to capture context and a symmetric expanding path that enables precise localization. We show that such a network can be trained end-to-end from very few images and outperforms the prior best method (a sliding-window convolutional network) on the ISBI challenge for segmentation of neuronal structures in electron microscopic stacks. Using the same network trained on transmitted light microscopy images (phase contrast and DIC) we won the ISBI cell tracking challenge 2015 in these categories by a large margin. Moreover, the network is fast. Segmentation of a 512x512 image takes less than a second on a recent GPU. The full implementation (based on Caffe) and the trained networks are available at http://lmb.informatik.uni-freiburg.de/people/ronneber/u-net.},
author = {Ronneberger, Olaf and Fischer, Philipp and Brox, Thomas},
booktitle = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
doi = {10.1007/978-3-319-24574-4_28},
isbn = {9783319245737},
issn = {16113349},
pages = {234--241},
title = {{U-net: Convolutional networks for biomedical image segmentation}},
volume = {9351},
year = {2015}
}
@article{Gers2000,
abstract = {Long short-term memory (LSTM; Hochreiter and Schmidhuber, 1997) can solve numerous tasks not solvable by previous learning algorithms for recurrent neural networks (RNNs). We identify a weakness of LSTM networks processing continual input streams that are not a priori segmented into subsequences with explicitly marked ends at which the network's internal state could be reset. Without resets, the state may grow indefinitely and eventually cause the network to break down. Our remedy is a novel, adaptive "forget gate" that enables an LSTM cell to learn to reset itself at appropriate times, thus releasing internal resources. We review illustrative benchmark problems on which standard LSTM outperforms other RNN algorithms. All algorithms (including LSTM) fail to solve continual versions of these problems. LSTM with forget gates, however, easily solves them, and in an elegant way.},
author = {Gers, Felix A. and Schmidhuber, J{\"{u}}rgen and Cummins, Fred},
doi = {10.1162/089976600300015015},
issn = {08997667},
journal = {Neural Computation},
number = {10},
pages = {2451--2471},
pmid = {11032042},
title = {{Learning to forget: Continual prediction with LSTM}},
volume = {12},
year = {2000}
}
@article{Thabtah2020,
abstract = {The advent of Big Data has ushered a new era of scientific breakthroughs. One of the common issues that affects raw data is class imbalance problem which refers to imbalanced distribution of values of the response variable. This issue is present in fraud detection, network intrusion detection, medical diagnostics, and a number of other fields where negatively labeled instances significantly outnumber positively labeled instances. Modern machine learning techniques struggle to deal with imbalanced data by focusing on minimizing the error rate for the majority class while ignoring the minority class. The goal of our paper is demonstrate the effects of class imbalance on classification models. Concretely, we study the impact of varying class imbalance ratios on classifier accuracy. By highlighting the precise nature of the relationship between the degree of class imbalance and the corresponding effects on classifier performance we hope to help researchers to better tackle the problem. To this end, we carry out extensive experiments using 10-fold cross validation on a large number of datasets. In particular, we determine that the relationship between the class imbalance ratio and the accuracy is convex.},
author = {Thabtah, Fadi and Hammoud, Suhel and Kamalov, Firuz and Gonsalves, Amanda},
doi = {10.1016/j.ins.2019.11.004},
issn = {00200255},
journal = {Information Sciences},
keywords = {Class imbalance,Classification,Data analysis,Machine learning,Statistical analysis,Supervised learning},
pages = {429--441},
title = {{Data imbalance in classification: Experimental evaluation}},
volume = {513},
year = {2020}
}
@misc{Hodge2004,
abstract = {Outlier detection has been used for centuries to detect and, where appropriate, remove anomalous observations from data. Outliers arise due to mechanical faults, changes in system behaviour, fraudulent behaviour, human error, instrument error or simply through natural deviations in populations. Their detection can identify system faults and fraud before they escalate with potentially catastrophic consequences. It can identify errors and remove their contaminating effect on the data set and as such to purify the data for processing. The original outlier detection methods were arbitrary but now, principled and systematic techniques are used, drawn from the full gamut of Computer Science and Statistics. In this paper, we introduce a survey of contemporary techniques for outlier detection. We identify their respective motivations and distinguish their advantages and disadvantages in a comparative review.},
author = {Hodge, Victoria J. and Austin, Jim},
booktitle = {Artificial Intelligence Review},
doi = {10.1023/B:AIRE.0000045502.10941.a9},
issn = {02692821},
keywords = {Anomaly,Detection,Deviation,Noise,Novelty,Outlier,Recognition},
number = {2},
pages = {85--126},
title = {{A survey of outlier detection methodologies}},
volume = {22},
year = {2004}
}
@inproceedings{Alansari2018,
abstract = {Health is one of the sustainable development areas in all of the countries. Internet of Things has a variety of use in this sector which was not studied yet. The aim of this research is to prioritize IoT usage in the healthcare sector to achieve sustainable development. The study is an applied descriptive research according to data collection. As per the research methodology which is FAHP, it is a single cross-sectional survey research. After data collection, the agreed paired comparison matrices, allocated to weighted criteria and the priority of IoT usage were determined. Based on the research findings, the two criteria of “Economic Prosperity” and “Quality of Life” achieved the highest priority for IoT sustainable development in the healthcare sector. Moreover, the top priorities for IoT in the area of health, according to the usage, were identified as “Ultraviolet Radiation,” “Dental Health,” and “Fall Detection.”.},
author = {Alansari, Zainab and Soomro, Safeeullah and Belgaum, Mohammad Riyaz and Shamshirband, Shahaboddin},
booktitle = {Advances in Intelligent Systems and Computing},
doi = {10.1007/978-981-10-6875-1_66},
isbn = {9789811068744},
issn = {21945357},
keywords = {Big data,Fuzzy-AHP,Healthcare,Internet of Things (IoT)},
pages = {675--685},
title = {{The rise of Internet of Things (IoT) in big healthcare data: Review and open research issues}},
volume = {564},
year = {2018}
}
@misc{Wen2019,
abstract = {Time series anomaly detection plays a critical role in automated monitoring systems. Most previous deep learning efforts related to time series anomaly detection were based on recurrent neural networks (RNN). In this paper, we propose a time series segmentation approach based on convolutional neural networks (CNN) for anomaly detection. Moreover, we propose a transfer learning framework that pretrains a model on a large-scale synthetic univariate time series data set and then fine-tunes its weights on small-scale, univariate or multivariate data sets with previously unseen classes of anomalies. For the multivariate case we introduce a novel network architecture. The approach was tested on multiple synthetic and real data sets successfully.},
archivePrefix = {arXiv},
arxivId = {1905.13628},
author = {Wen, Tailai and Keyes, Roy},
booktitle = {arXiv},
eprint = {1905.13628},
issn = {23318422},
title = {{Time Series Anomaly Detection Using Convolutional Neural Networks and Transfer Learning}},
year = {2019}
}
@article{Hauskrecht2007,
abstract = {Anomaly detection methods can be very useful in identifying interesting or concerning events. In this work, we develop and examine new probabilistic anomaly detection methods that let us evaluate management decisions for a specific patient and identify those decisions that are highly unusual with respect to patients with the same or similar condition. The statistics used in this detection are derived from probabilistic models such as Bayesian networks that are learned from a database of past patient cases. We evaluate our methods on the problem of detection of unusual hospitalization patterns for patients with community acquired pneumonia. The results show very encouraging detection performance with 0.5 precision at 0.53 recall and give us hope that these techniques may provide the basis of intelligent monitoring systems that alert clinicians to the occurrence of unusual events or decisions.},
author = {Hauskrecht, Milos and Valko, Michal and Kveton, Branislav and Visweswaran, Shyam and Cooper, Gregory F.},
issn = {15594076},
journal = {AMIA ... Annual Symposium proceedings / AMIA Symposium. AMIA Symposium},
pages = {319--323},
pmid = {18693850},
title = {{Evidence-based anomaly detection in clinical domains.}},
year = {2007}
}
