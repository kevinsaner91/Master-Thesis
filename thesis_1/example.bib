@article{Hauskrecht2007,
   abstract = {Anomaly detection methods can be very useful in identifying interesting or concerning events. In this work, we develop and examine new probabilistic anomaly detection methods that let us evaluate management decisions for a specific patient and identify those decisions that are highly unusual with respect to patients with the same or similar condition. The statistics used in this detection are derived from probabilistic models such as Bayesian networks that are learned from a database of past patient cases. We evaluate our methods on the problem of detection of unusual hospitalization patterns for patients with community acquired pneumonia. The results show very encouraging detection performance with 0.5 precision at 0.53 recall and give us hope that these techniques may provide the basis of intelligent monitoring systems that alert clinicians to the occurrence of unusual events or decisions.},
   author = {Milos Hauskrecht and Michal Valko and Branislav Kveton and Shyam Visweswaran and Gregory F. Cooper},
   issn = {15594076},
   journal = {AMIA ... Annual Symposium proceedings / AMIA Symposium.},
   pages = {319-323},
   pmid = {18693850},
   title = {Evidence-based anomaly detection in clinical domains},
   year = {2007},
}
@article{Wen2019,
   abstract = {Time series anomaly detection plays a critical role in automated monitoring systems. Most previous deep learning efforts related to time series anomaly detection were based on recurrent neural networks (RNN). In this paper, we propose a time series segmentation approach based on convolutional neural networks (CNN) for anomaly detection. Moreover, we propose a transfer learning framework that pretrains a model on a large-scale synthetic univariate time series data set and then fine-tunes its weights on small-scale, univariate or multivariate data sets with previously unseen classes of anomalies. For the multivariate case we introduce a novel network architecture. The approach was tested on multiple synthetic and real data sets successfully.},
   author = {Tailai Wen and Roy Keyes},
   issn = {23318422},
   journal   = {Computing Research Repository},
   volume    = {abs/1905.13628},
   title = {Time Series Anomaly Detection Using Convolutional Neural Networks and Transfer Learning},
   year = {2019},
}
@article{Tsay2000,
	author = {Tsay, Ruey S. and Peña, Daniel and Pankratz, Alan E.},
	title = "{Outliers in multivariate time series}",
	journal = {Biometrika},
	volume = {87},
	issue = {4},
	pages = {789-804},
	year = {2000},
	month = {12},
	abstract = "{This paper generalises four types of disturbance commonly used in univariate time series analysis to the multivariate case, highlights the differences between univariate and multivariate outliers, and investigates dynamic effects of a multivariate outlier on individual components. The effect of a multivariate outlier depends not only on its size and the underlying model, but also on the interaction between the size and the dynamic structure of the model. The latter factor does not appear in the univariate case. A multivariate outlier can introduce various types of outlier for the marginal component models. By comparing and contrasting results of univariate and multivariate outlier detections, one can gain insights into the characteristics of an outlier. We use real examples to demonstrate the proposed analysis.}",
	issn = {0006-3444},
	doi = {10.1093/biomet/87.4.789},
}
@inproceedings{Alansari2018,
   abstract = {Health is one of the sustainable development areas in all of the countries. Internet of Things has a variety of use in this sector which was not studied yet. The aim of this research is to prioritize IoT usage in the healthcare sector to achieve sustainable development. The study is an applied descriptive research according to data collection. As per the research methodology which is FAHP, it is a single cross-sectional survey research. After data collection, the agreed paired comparison matrices, allocated to weighted criteria and the priority of IoT usage were determined. Based on the research findings, the two criteria of “Economic Prosperity” and “Quality of Life” achieved the highest priority for IoT sustainable development in the healthcare sector. Moreover, the top priorities for IoT in the area of health, according to the usage, were identified as “Ultraviolet Radiation,” “Dental Health,” and “Fall Detection.”.},
   author = {Zainab Alansari and Safeeullah Soomro and Mohammad Riyaz Belgaum and Shahaboddin Shamshirband},
   doi = {10.1007/978-981-10-6875-1_66},
   isbn = {978-3-319-45990-5},
   issn = {21945357},
   editor = {Natalya Shakhovska},
   publisher = {Springer, Cham},
   booktitle = {Advances in Intelligent Systems and Computing},
   journal = {Advances in Intelligent Systems and Computing},
   keywords = {Big data,Fuzzy-AHP,Healthcare,Internet of Things (IoT)},
   pages = {675-685},
   title = {The rise of Internet of Things (IoT) in big healthcare data: Review and open research issues},
   volume = {564},
   year = {2018},
}

@article{Hodge2004,
   abstract = {Outlier detection has been used for centuries to detect and, where appropriate, remove anomalous observations from data. Outliers arise due to mechanical faults, changes in system behaviour, fraudulent behaviour, human error, instrument error or simply through natural deviations in populations. Their detection can identify system faults and fraud before they escalate with potentially catastrophic consequences. It can identify errors and remove their contaminating effect on the data set and as such to purify the data for processing. The original outlier detection methods were arbitrary but now, principled and systematic techniques are used, drawn from the full gamut of Computer Science and Statistics. In this paper, we introduce a survey of contemporary techniques for outlier detection. We identify their respective motivations and distinguish their advantages and disadvantages in a comparative review.},
   author = {Victoria J. Hodge and Jim Austin},
   doi = {10.1023/B:AIRE.0000045502.10941.a9},
   issn = {02692821},
   issue = {2},
   journal = {Artificial Intelligence Review},
   pages = {85-126},
   title = {A survey of outlier detection methodologies},
   volume = {22},
   year = {2004},
}
@article{Thabtah2020,
   abstract = {The advent of Big Data has ushered a new era of scientific breakthroughs. One of the common issues that affects raw data is class imbalance problem which refers to imbalanced distribution of values of the response variable. This issue is present in fraud detection, network intrusion detection, medical diagnostics, and a number of other fields where negatively labeled instances significantly outnumber positively labeled instances. Modern machine learning techniques struggle to deal with imbalanced data by focusing on minimizing the error rate for the majority class while ignoring the minority class. The goal of our paper is demonstrate the effects of class imbalance on classification models. Concretely, we study the impact of varying class imbalance ratios on classifier accuracy. By highlighting the precise nature of the relationship between the degree of class imbalance and the corresponding effects on classifier performance we hope to help researchers to better tackle the problem. To this end, we carry out extensive experiments using 10-fold cross validation on a large number of datasets. In particular, we determine that the relationship between the class imbalance ratio and the accuracy is convex.},
   author = {Fadi Thabtah and Suhel Hammoud and Firuz Kamalov and Amanda Gonsalves},
   doi = {10.1016/j.ins.2019.11.004},
   issn = {00200255},
   journal = {Information Sciences},
   pages = {429-441},
   title = {Data imbalance in classification: Experimental evaluation},
   volume = {513},
   year = {2020},
}
@article{Gers2000,
   abstract = {Long short-term memory (LSTM; Hochreiter and Schmidhuber, 1997) can solve numerous tasks not solvable by previous learning algorithms for recurrent neural networks (RNNs). We identify a weakness of LSTM networks processing continual input streams that are not a priori segmented into subsequences with explicitly marked ends at which the network's internal state could be reset. Without resets, the state may grow indefinitely and eventually cause the network to break down. Our remedy is a novel, adaptive "forget gate" that enables an LSTM cell to learn to reset itself at appropriate times, thus releasing internal resources. We review illustrative benchmark problems on which standard LSTM outperforms other RNN algorithms. All algorithms (including LSTM) fail to solve continual versions of these problems. LSTM with forget gates, however, easily solves them, and in an elegant way.},
   author = {Felix A. Gers and Jürgen Schmidhuber and Fred Cummins},
   doi = {10.1162/089976600300015015},
   issn = {08997667},
   issue = {10},
   journal = {Neural Computation},
   pages = {2451-2471},
   pmid = {11032042},
   title = {Learning to forget: Continual prediction with LSTM},
   volume = {12},
   year = {2000},
}
@inproceedings{Ronneberger2015,
   abstract = {There is large consent that successful training of deep networks requires many thousand annotated training samples. In this paper, we present a network and training strategy that relies on the strong use of data augmentation to use the available annotated samples more efficiently. The architecture consists of a contracting path to capture context and a symmetric expanding path that enables precise localization. We show that such a network can be trained end-to-end from very few images and outperforms the prior best method (a sliding-window convolutional network) on the ISBI challenge for segmentation of neuronal structures in electron microscopic stacks. Using the same network trained on transmitted light microscopy images (phase contrast and DIC) we won the ISBI cell tracking challenge 2015 in these categories by a large margin. Moreover, the network is fast. Segmentation of a 512x512 image takes less than a second on a recent GPU. The full implementation (based on Caffe) and the trained networks are available at http://lmb.informatik.uni-freiburg.de/people/ronneber/u-net.},
   author = {Olaf Ronneberger and Philipp Fischer and Thomas Brox},
   doi = {10.1007/978-3-319-24574-4_28},
   isbn = {9783319245737},
   issn = {16113349},
   editor = { Navab N. and Hornegger J. and Wells W. and Frangi A.},
   booktitle = {Medical Image Computing and Computer-Assisted Intervention – MICCAI 2015. MICCAI 2015. Lecture Notes in Computer Science}, 
   journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
   pages = {234-241},
   title = {U-net: Convolutional networks for biomedical image segmentation},
   volume = {9351},
   publisher = {Springer},
   address = {Cham},
   year = {2015},
}
@article{Shelhamer2017,
   abstract = {Convolutional networks are powerful visual models that yield hierarchies of features. We show that convolutional networks by themselves, trained end-to-end, pixels-to-pixels, improve on the previous best result in semantic segmentation. Our key insight is to build 'fully convolutional' networks that take input of arbitrary size and produce correspondingly-sized output with efficient inference and learning. We define and detail the space of fully convolutional networks, explain their application to spatially dense prediction tasks, and draw connections to prior models. We adapt contemporary classification networks (AlexNet, the VGG net, and GoogLeNet) into fully convolutional networks and transfer their learned representations by fine-tuning to the segmentation task. We then define a skip architecture that combines semantic information from a deep, coarse layer with appearance information from a shallow, fine layer to produce accurate and detailed segmentations. Our fully convolutional networks achieve improved segmentation of PASCAL VOC (30% relative improvement to 67.2% mean IU on 2012), NYUDv2, SIFT Flow, and PASCAL-Context, while inference takes one tenth of a second for a typical image.},
   author = {Evan Shelhamer and Jonathan Long and Trevor Darrell},
   doi = {10.1109/TPAMI.2016.2572683},
   issn = {01628828},
   issue = {4},
   journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
   keywords = {Convolutional Networks,Deep Learning,Semantic Segmentation,Transfer Learning},
   pages = {640-651},
   pmid = {27244717},
   title = {Fully Convolutional Networks for Semantic Segmentation},
   volume = {39},
   year = {2017},
}
@inproceedings{Dutta2018,
   abstract = {The success of deep learning based models have centered around recent architectures and the availability of large scale annotated data. In this work, we explore these two factors systematically for improving handwritten recognition for scanned off-line document images. We propose a modified CNN-RNN hybrid architecture with a major focus on effective training using: (i) efficient initialization of network using synthetic data for pretraining, (ii) image normalization for slant correction and (iii) domain specific data transformation and distortion for learning important invariances. We perform a detailed ablation study to analyze the contribution of individual modules and present state of art results for the task of unconstrained line and word recognition on popular datasets such as IAM, RIMES and GW.},
   author = {Kartik Dutta and Praveen Krishnan and Minesh Mathew and C. V. Jawahar},
   doi = {10.1109/ICFHR-2018.2018.00023},
   isbn = {978-1-5386-5875-8},
   issn = {21676453},
   booktitle={2018 16th International Conference on Frontiers in Handwriting Recognition (ICFHR)}, 
   pages = {80-85},
   publisher = {IEEE},
   location = {Niagara Falls, NY, USA},
   title = {Improving CNN-RNN hybrid networks for handwriting recognition},
   year = {2018},
}
@inproceedings{Fan2016,
   abstract = {In this paper, we present a video-based emotion recognition system submitted to the EmotiW 2016 Challenge. The core module of this system is a hybrid network that combines recurrent neural network (RNN) and 3D convolutional networks (C3D) in a late-fusion fashion. RNN and C3D encode appearance and motion information in different ways. Specifically, RNN takes appearance features extracted by convolutional neural network (CNN) over individual video frames as input and encodes motion later, while C3D models appearance and motion of video simultaneously. Combined with an audio module, our system achieved a recognition accuracy of 59.02% without using any additional emotion-labeled video clips in training set, compared to 53.8% of the winner of EmotiW 2015. Extensive experiments show that combining RNN and C3D together can improve video-based emotion recognition noticeably.},
   author = {Yin Fan and Xiangju Lu and Dian Li and Yuanliu Liu},
   doi = {10.1145/2993148.2997632},
   isbn = {9781450345569},
   journal = {ICMI 2016 - Proceedings of the 18th ACM International Conference on Multimodal Interaction},
   booktitle = {ICMI 2016 - Proceedings of the 18th ACM International Conference on Multimodal Interaction},
   pages = {445-450},
   title = {Video-Based emotion recognition using CNN-RNN and C3D hybrid networks},
   publisher = {Association for Computing Machinery},
   location = {Tokyo, Japan},
   year = {2016},
}
@article{JunyoungChung2014,
   abstract = {In this paper we compare different types of recurrent units in recurrent neural networks (RNNs). Especially, we focus on more sophisticated units that implement a gating mechanism, such as a long short-term memory (LSTM) unit and a recently proposed gated recurrent unit (GRU). We evaluate these recurrent units on the tasks of polyphonic music modeling and speech signal modeling. Our experiments revealed that these advanced recurrent units are indeed better than more traditional recurrent units such as tanh units. Also, we found GRU to be comparable to LSTM.},
   author = {Junyoung Chung and Caglar Gulcehre and KyungHyun Cho and Yoshua Bengio},
   journal = {Computing Research Repository},
   month = {12},
   title = {Empirical Evaluation of Gated Recurrent Neural Networks on Sequence Modeling},
   url = {http://arxiv.org/abs/1412.3555},
   year = {2014},
}
@article{Hochreiter1997,
   abstract = {Learning to store information over extended time intervals by recurrent backpropagation takes a very long time, mostly because of insufficient, decaying error backflow. We briefly review Hochreiter's (1991) analysis of this problem, then address it by introducing a novel, efficient, gradient based method called long short-term memory (LSTM). Truncating the gradient where this does not do harm, LSTM can learn to bridge minimal time lags in excess of 1000 discrete-time steps by enforcing constant error flow through constant error carousels within special units. Multiplicative gate units learn to open and close access to the constant error flow. LSTM is local in space and time; its computational complexity per time step and weight is O. 1. Our experiments with artificial data involve local, distributed, real-valued, and noisy pattern representations. In comparisons with real-time recurrent learning, back propagation through time, recurrent cascade correlation, Elman nets, and neural sequence chunking, LSTM leads to many more successful runs, and learns much faster. LSTM also solves complex, artificial long-time-lag tasks that have never been solved by previous recurrent network algorithms.},
   author = {Sepp Hochreiter and Jürgen Schmidhuber},
   doi = {10.1162/neco.1997.9.8.1735},
   issn = {0899-7667},
   issue = {8},
   journal = {Neural Computation },
   month = {11},
   pages = {1735-1780},
   title = {Long Short-Term Memory},
   volume = {9},
   year = {1997},
}
@web_page{DennyBritz2015,
   author = {Denny Britz},
   journal = {WILDML  Artificial Intelligence, Deep Learning, and NLP},
   month = {9},
   title = {Implementing a Neural Network from Scratch in Python – An Introduction},
   url = {http://www.wildml.com/2015/09/implementing-a-neural-network-from-scratch/},
   year = {2015},
}
@web_page{MichaelPhi2018,
   author = {Michael Phi},
   journal = {Towards Data Science},
   month = {9},
   title = {Illustrated Guide to LSTM’s and GRU’s: A step by step explanation},
   url = {https://towardsdatascience.com/illustrated-guide-to-lstms-and-gru-s-a-step-by-step-explanation-44e9eb85bf21},
   year = {2018},
}
@web_page{RichStureborg2019,
   author = {Rich Stureborg},
   journal = {Towards Data Science},
   month = {1},
   pages = {-undefined},
   title = {Conv Nets for dummies},
   url = {https://towardsdatascience.com/conv-nets-for-dummies-a-bottom-up-approach-c1b754fb14d6},
   year = {2019},
}
@web_page{NiklasDonges2020,
   author = {Niklas Donges},
   journal = {Built-In},
   month = {9},
   title = {What is transfer learning? Exploring the popular deep learning approach},
   url = {https://builtin.com/data-science/transfer-learning},
   year = {2020},
}
@article{Braei2020,
   abstract = {Anomaly detection for time-series data has been an important research field for a long time. Seminal work on anomaly detection methods has been focussing on statistical approaches. In recent years an increasing number of machine learning algorithms have been developed to detect anomalies on time-series. Subsequently, researchers tried to improve these techniques using (deep) neural networks. In the light of the increasing number of anomaly detection methods, the body of research lacks a broad comparative evaluation of statistical, machine learning and deep learning methods. This paper studies 20 univariate anomaly detection methods from the all three categories. The evaluation is conducted on publicly available datasets, which serve as benchmarks for time-series anomaly detection. By analyzing the accuracy of each method as well as the computation time of the algorithms, we provide a thorough insight about the performance of these anomaly detection approaches, alongside some general notion of which method is suited for a certain type of data.},
   author = {Mohammad Braei and Sebastian Wagner},
   journal = {Computing Research Repository},
   volume    = {abs/2004.00433},
   title = {Anomaly detection in univariate time-series: A survey on the state-of-the-art},
   year = {2020},
}
@book{Aggarwal2013,
	abstract = {With the increasing advances in hardware technology for data collection, and advances in software technology (databases) for data organization, computer scientists have increasingly participated in the latest advancements of the outlier analysis field. Computer scientists, specifically, approach this field based on their practical experiences in managing large amounts of data, and with far fewer assumptions- the data can be of any type, structured or unstructured, and may be extremely large. Outlier Analysis is a comprehensive exposition, as understood by data mining experts, statisticians and computer scientists. The book has been organized carefully, and emphasis was placed on simplifying the content, so that students and practitioners can also benefit. Chapters will typically cover one of three areas: methods and techniques commonly used in outlier analysis, such as linear methods, proximity-based methods, subspace methods, and supervised methods; data domains, such as, text, categorical, mixed-attribute, time-series, streaming, discrete sequence, spatial and network data; and key applications of these methods as applied to diverse domains such as credit card fraud detection, intrusion detection, medical diagnosis, earth science, web log analytics, and social network analysis are covered.},
	author = {Charu C. Aggarwal},
	city = {New York},
	doi = {10.1007/978-1-4614-6396-2},
	isbn = {978-1-4614-6396-2},
	booktitel = {Outlier Analysis},
	pages = {1-446},
	publisher = {Springer, New York},
	title = {Outlier analysis},
	year = {2013},
}
@inbook{Ord1996,
   abstract = {No abstract is available for this item.},
   author = {Keith Ord},
   issue = {1},
   booktitle = {International Journal of Forecasting},
   journal = {International Journal of Forecasting},
   title = {Outliers in statistical data},
   volume = {12},
   isbn = {ISBN 0-471-93094-6},
   publisher = {John Wiley and Sons},
   address = {Chichester},
   year = {1996},
}
@inproceedings{Chandola2009,
	author = {Arindam Chandola, Varun, Banerjee and Vipin Kumar},
	doi = {https://doi.org/10.1145},
	issue = {3},
	journal = {ACM computing surveys (CSUR)},
	Booktitle = {ACM computing surveys (CSUR)},
	pages = {1-58},
	publisher = {Association for Computing Machinery},
	address = {New York, USA},
	title = {Anomaly detection: A survey},
	volume = {41},
	number = {3},
	year = {2009},
}

@inproceedings{Malhotra2015,
   abstract = {Long Short Term Memory (LSTM) networks have been demonstrated to be particularly useful for learning sequences containing longer term patterns of unknown length, due to their ability to maintain long term memory. Stacking recurrent hidden layers in such networks also enables the learning of higher level temporal features, for faster learning with sparser representations. In this paper, we use stacked LSTM networks for anomaly/fault detection in time series. A network is trained on non-anomalous data and used as a predictor over a number of time steps. The resulting prediction errors are modeled as a multivariate Gaussian distribution, which is used to assess the likelihood of anomalous behavior. The efficacy of this approach is demonstrated on four datasets: ECG, space shuttle, power demand, and multi-sensor engine dataset.},
   author = {Pankaj Malhotra and Lovekesh Vig and Gautam Shroff and Puneet Agarwal},
   isbn = {9782875870148},
   booktitle = {23rd European Symposium on Artificial Neural Networks, Computational Intelligence and Machine Learning, ESANN 2015 - Proceedings},
   journal = {23rd European Symposium on Artificial Neural Networks, Computational Intelligence and Machine Learning, ESANN 2015 - Proceedings},
   pages = {89-94},
   publisher = {i6doc.com},
   location = {Bruges, Belgium},
   title = {Long Short Term Memory networks for anomaly detection in time series},
   year = {2015},
}
@inproceedings{Chauhan2015,
   abstract = {Electrocardiography (ECG) signals are widely used to gauge the health of the human heart, and the resulting time series signal is often analyzed manually by a medical professional to detect any arrhythmia that the patient may have suffered. Much work has been done to automate the process of analyzing ECG signals, but most of the research involves extensive preprocessing of the ECG data to derive vectorized features and subsequently designing a classifier to discriminate between healthy ECG signals and those indicative of an Arrhythmia. This approach requires knowledge and data of the different types of Arrhythmia for training. However, the heart is a complex organ and there are many different and new types of Arrhythmia that can occur which were not part of the original training set. Thus, it may be more prudent to adopt an anomaly detection approach towards analyzing ECG signals. In this paper, we utilize a deep recurrent neural network architecture with Long Short Term Memory (LSTM) units to develop a predictive model for healthy ECG signals. We further utilize the probability distribution of the prediction errors from these recurrent models to indicate normal or abnormal behavior. An added advantage of using LSTM networks is that the ECG signal can be directly fed into the network without any elaborate preprocessing as required by other techniques. Also, no prior information about abnormal signals is needed by the networks as they were trained only on normal data. We have used the MIT-BIH Arrhythmia Database to obtain ECG time series data for both normal periods and for periods during four different types of Arrhythmias, namely Premature Ventricular Contraction (PVC), Atrial Premature Contraction (APC), Paced Beats (PB) and Ventricular Couplet (VC). Results are promising and indicate that Deep LSTM models may be viable for detecting anomalies in ECG signals.},
   author = {Sucheta Chauhan and Lovekesh Vig},
   doi = {10.1109/DSAA.2015.7344872},
   isbn = {9781467382731},
   journal = {Proceedings of the 2015 IEEE International Conference on Data Science and Advanced Analytics, DSAA 2015},
   title = {Anomaly detection in ECG time signals via deep long short-term memory networks},
   year = {2015},
}
@inproceedings{Zheng2014,
   abstract = {Time series (particularly multivariate) classification has drawn a lot of attention in the literature because of its broad applications for different domains, such as health informatics and bioinformatics. Thus, many algorithms have been developed for this task. Among them, nearest neighbor classification (particularly 1-NN) combined with Dynamic Time Warping (DTW) achieves the state of the art performance. However, when data set grows larger, the time consumption of 1-NN with DTW grows linearly. Compared to 1-NN with DTW, the traditional feature-based classification methods are usually more efficient but less effective since their performance is usually dependent on the quality of hand-crafted features. To that end, in this paper, we explore the feature learning techniques to improve the performance of traditional feature-based approaches. Specifically, we propose a novel deep learning framework for multivariate time series classification. We conduct two groups of experiments on real-world data sets from different application domains. The final results show that our model is not only more efficient than the state of the art but also competitive in accuracy. It also demonstrates that feature learning is worth to investigate for time series classification. © 2014 Springer International Publishing Switzerland.},
   author = {Yi Zheng and Qi Liu and Enhong Chen and Yong Ge and J. Leon Zhao},
   doi = {10.1007/978-3-319-08010-9_33},
   isbn = {9783319080093},
   issn = {16113349},
   booktitle = {Web-Age Information Management. WAIM 2014. Lecture Notes in Computer Science},
   journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
   pages = {298-310},
   editor = {Li F. and Li G. and Hwang S. and Yao B. and Zhang Z.},
   title = {Time series classification using multi-channels deep convolutional neural networks},
   publisher = {Springer},
   address = {Cham},
   volume = {8485},
   year = {2014},
}
@inproceedings{Cicek2016,
   abstract = {This paper introduces a network for volumetric segmentation that learns from sparsely annotated volumetric images. We outline two attractive use cases of this method: (1) In a semi-automated setup,the user annotates some slices in the volume to be segmented. The network learns from these sparse annotations and provides a dense 3D segmentation. (2) In a fully-automated setup,we assume that a representative,sparsely annotated training set exists. Trained on this data set,the network densely segments new volumetric images. The proposed network extends the previous u-net architecture from Ronneberger et al. by replacing all 2D operations with their 3D counterparts. The implementation performs on-the-fly elastic deformations for efficient data augmentation during training. It is trained end-to-end from scratch,i.e.,no pre-trained network is required. We test the performance of the proposed method on a complex,highly variable 3D structure,the Xenopus kidney,and achieve good results for both use cases.},
   author = {Özgün Çiçek and Ahmed Abdulkadir and Soeren S. Lienkamp and Thomas Brox and Olaf Ronneberger},
   doi = {10.1007/978-3-319-46723-8_49},
   issn = {16113349},
   editor = {Ourselin S. and Joskowicz L. and Sabuncu M. and Unal G. and Wells W. },
   booktitle = {Medical Image Computing and Computer-Assisted Intervention – MICCAI 2016. MICCAI 2016. Lecture Notes in Computer Science},
   journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
   title = {3D U-net: Learning dense volumetric segmentation from sparse annotation},
   publisher = {Springer},
   address = {Cham},
   volume = {9901},
   year = {2016},
}
@article{Munir2019,
   abstract = {Traditional distance and density-based anomaly detection techniques are unable to detect periodic and seasonality related point anomalies which occur commonly in streaming data, leaving a big gap in time series anomaly detection in the current era of the IoT. To address this problem, we present a novel deep learning-based anomaly detection approach (DeepAnT) for time series data, which is equally applicable to the non-streaming cases. DeepAnT is capable of detecting a wide range of anomalies, i.e., point anomalies, contextual anomalies, and discords in time series data. In contrast to the anomaly detection methods where anomalies are learned, DeepAnT uses unlabeled data to capture and learn the data distribution that is used to forecast the normal behavior of a time series. DeepAnT consists of two modules: time series predictor and anomaly detector. The time series predictor module uses deep convolutional neural network (CNN) to predict the next time stamp on the defined horizon. This module takes a window of time series (used as a context) and attempts to predict the next time stamp. The predicted value is then passed to the anomaly detector module, which is responsible for tagging the corresponding time stamp as normal or abnormal. DeepAnT can be trained even without removing the anomalies from the given data set. Generally, in deep learning-based approaches, a lot of data are required to train a model. Whereas in DeepAnT, a model can be trained on relatively small data set while achieving good generalization capabilities due to the effective parameter sharing of the CNN. As the anomaly detection in DeepAnT is unsupervised, it does not rely on anomaly labels at the time of model generation. Therefore, this approach can be directly applied to real-life scenarios where it is practically impossible to label a big stream of data coming from heterogeneous sensors comprising of both normal as well as anomalous points. We have performed a detailed evaluation of 15 algorithms on 10 anomaly detection benchmarks, which contain a total of 433 real and synthetic time series. Experiments show that DeepAnT outperforms the state-of-the-art anomaly detection methods in most of the cases, while performing on par with others.},
   author = {Mohsin Munir and Shoaib Ahmed Siddiqui and Andreas Dengel and Sheraz Ahmed},
   doi = {10.1109/ACCESS.2018.2886457},
   issn = {21693536},
   journal = {IEEE },
   keywords = {Anomaly detection,artificial intelligence,convolutional neural network,deep neural networks,recurrent neural networks,time series analysis},
   pages = {1991-2005},
   title = {DeepAnT: A Deep Learning Approach for Unsupervised Anomaly Detection in Time Series},
   volume = {7},
   year = {2019},
}
@article{LeCun1998,
   abstract = {Multilayer neural networks trained with the back-propagation algorithm constitute the best example of a successful gradient-based learning technique. Given an appropriate network architecture, gradient-based learning algorithms can be used to synthesize a complex decision surface that can classify high-dimensional patterns, such as handwritten characters, with minimal preprocessing. This paper reviews various methods applied to handwritten character recognition and compares them on a standard handwritten digit recognition task. Convolutional neural networks, which are specifically designed to deal with the variability of two dimensional (2-D) shapes, are shown to outperform all other techniques. Real-life document recognition systems are composed of multiple modules including field extraction, segmentation, recognition, and language modeling. A new learning paradigm, called graph transformer networks (GTN's), allows such multimodule systems to be trained globally using gradient-based methods so as to minimize an overall performance measure. Two systems for online handwriting recognition are described. Experiments demonstrate the advantage of global training, and the flexibility of graph transformer networks. A graph transformer network for reading a bank check is also described. It uses convolutional neural network character recognizers combined with global training techniques to provide record accuracy on business and personal checks. It is deployed commercially and reads several million checks per day. © 1998 IEEE.},
   author = {Yann LeCun and Léon Bottou and Yoshua Bengio and Patrick Haffner},
   doi = {10.1109/5.726791},
   issn = {00189219},
   issue = {11},
   journal = {Proceedings of the IEEE},
   keywords = {Convolutional neural networks,Document recognition,Finite state transducers,Gradient-based learning,Graph transformer networks,Machine learning,Neural networks,Optical character recognition (OCR)},
   pages = {2278-2323},
   title = {Gradient-based learning applied to document recognition},
   volume = {86},
   year = {1998},
}
@generic{Gupta2018,
   abstract = {Deep neural networks have shown promising results for various clinical prediction tasks such as diagnosis, mortality prediction, predicting duration of stay in hospital, etc. However, training deep networks - such as those based on Recurrent Neural Networks (RNNs) - requires large labeled data, high computational resources, and significant hyperparameter tuning effort. In this work, we investigate as to what extent can transfer learning address these issues when using deep RNNs to model multivariate clinical time series. We consider transferring the knowledge captured in an RNN trained on several source tasks simultaneously using a large labeled dataset to build the model for a target task with limited labeled data. An RNN pre-trained on several tasks provides generic features, which are then used to build simpler linear models for new target tasks without training task-specific RNNs. For evaluation, we train a deep RNN to identify several patient phenotypes on time series from MIMIC-III database, and then use the features extracted using that RNN to build classifiers for identifying previously unseen phenotypes, and also for a seemingly unrelated task of in-hospital mortality. We demonstrate that (i) models trained on features extracted using pre-trained RNN outperform or, in the worst case, perform as well as task-specific RNNs; (ii) the models using features from pre-trained models are more robust to the size of labeled data than task-specific RNNs; and (iii) features extracted using pre-trained RNN are generic enough and perform better than typical statistical hand-crafted features.},
   author = {Priyanka Gupta and Pankaj Malhotra and Lovekesh Vig and Gautam Shroff},
   issn = {23318422},
   journal = {arXiv},
   title = {Transfer learning for clinical time series analysis using recurrent neural networks},
   year = {2018},
}
@book{Chollet2018,
   author = {François Chollet and Joseph J. Allaire},
   isbn = {9781617295546},
   pages = {1-360},
   publisher = {Manning Publications},
   title = {Deep Learning with R},
   year = {2018},
}
@inproceedings{Graves2005,
   abstract = {In this paper, we carry out two experiments on the TIMIT speech corpus with bidirectional and unidirectional Long Short Term Memory (LSTM) networks. In the first experiment (framewise phoneme classification) we find that bidirectional LSTM outperforms both unidirectional LSTM and conventional Recurrent Neural Networks (RNNs). In the second (phoneme recognition) we find that a hybrid BLSTM-HMM system improves on an equivalent traditional HMM system, as well as unidirectional LSTM-HMM. © Springer-Verlag Berlin Heidelberg 2005.},
   author = {Alex Graves and Santiago Fernández and Jürgen Schmidhuber},
   doi = {10.1007/11550907_126},
   issn = {03029743},
   journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
   booktitle = {Artificial Neural Networks: Formal Models and Their Applications – ICANN 2005},
   title = {Bidirectional LSTM networks for improved phoneme classification and recognition},
   publisher = {Springer},
   editor = {Duch W. and Kacprzyk J. and Oja E. and Zadrożny S.},
   address = {Berlin, Heidelberg},
   volume = {3697},
   year = {2005},
}
@thesis{Verner2019,
   author = {Alexander Verner},
   institution = {Doctoral dissertation. Nova Southeastern University},
   title = {LSTM Networks for Detection and Classification
of Anomalies in Raw Sensor Data},
   url = {https://nsuworks.nova.edu/gscis_etd/1074},
   location = {California},
   year = {2019},
}

@article{Wu2020,
   author = {Renjie Wu and Eamonn J. Keogh},
   title = {Current Time Series Anomaly Detection Benchmarks are Flawed and are Creating the Illusion of Progress},
   journal={IEEE Transactions on Knowledge and Data Engineering},
   year = {2020},
   doi={10.1109/TKDE.2021.3112126}
}


@article{Filonov2016,
   abstract = {We adopted an approach based on an LSTM neural network to monitor and detect faults in industrial multivariate time series data. To validate the approach we created a Modelica model of part of a real gasoil plant. By introducing hacks into the logic of the Modelica model, we were able to generate both the roots and causes of fault behavior in the plant. Having a self-consistent data set with labeled faults, we used an LSTM architecture with a forecasting error threshold to obtain precision and recall quality metrics. The dependency of the quality metric on the threshold level is considered. An appropriate mechanism such as "one handle" was introduced for filtering faults that are outside of the plant operator field of interest.},
   author = {Filonov, Pavel and Lavrentyev, Andrey and Vorontsov, Artem },
   month = {12},
   journal   = {Computing Research Repository},
   title = {Multivariate Industrial Time Series with Cyber-Attack Simulation: Fault Detection Using an LSTM-based Predictive Data Model},
   year = {2016},
}
@article{Rosenblatt1958,
	author = {F. Rosenblatt},
	doi = {10.1037/h0042519},
	issn = {1939-1471},
	issue = {6},
	journal = {Psychological Review},
	title = {The perceptron: A probabilistic model for information storage and organization in the brain.},
	volume = {65},
	year = {1958},
}
@web_page{Katanforoosh2019,
	author = {Kian Katanforoosh and Daniel Kunin and Jiaju Ma},
	title = {Parameter optimization in neural networks},
	url = {https://www.deeplearning.ai/ai-notes/optimization/},
	year = {2019},
}
@article{Foorthuis2021,
	abstract = {Anomalies are occurrences in a dataset that are in some way unusual and do not fit the general patterns. The concept of the anomaly is typically ill defined and perceived as vague and domain-dependent. Moreover, despite some 250 years of publications on the topic, no comprehensive and concrete overviews of the different types of anomalies have hitherto been published. By means of an extensive literature review this study therefore offers the first theoretically principled and domain-independent typology of data anomalies and presents a full overview of anomaly types and subtypes. To concretely define the concept of the anomaly and its different manifestations, the typology employs five dimensions: data type, cardinality of relationship, anomaly level, data structure, and data distribution. These fundamental and data-centric dimensions naturally yield 3 broad groups, 9 basic types, and 63 subtypes of anomalies. The typology facilitates the evaluation of the functional capabilities of anomaly detection algorithms, contributes to explainable data science, and provides insights into relevant topics such as local versus global anomalies.},
	author = {Ralph Foorthuis},
	doi = {10.1007/s41060-021-00265-1},
	issn = {2364-415X},
	issue = {4},
	pages = {297–331},
	journal = {International Journal of Data Science and Analytics},
	month = {10},
	title = {On the nature and types of anomalies: a review of deviations in data},
	volume = {12},
	year = {2021},
}
@web_page{Google2021,
	author = {Google},
	title = {ROC and AUC},
	url = {https://developers.google.com/machine-learning/crash-course/classification/roc-and-auc},
	year = {2021},
}
@web_page{Lendave2021,
	author = {Vijaysinh Lendave},
	month = {8},
	title = {LSTM Vs GRU in Recurrent Neural Network: A Comparative Study },
	url = {https://analyticsindiamag.com/lstm-vs-gru-in-recurrent-neural-network-a-comparative-study/},
	year = {2021},
}
@web_page{Krishnan2019,
	author = {Bipin Krishnan},
	month = {11},
	title = {WHEN and WHY are batches used in machine learning ?},
	url = {https://medium.com/analytics-vidhya/when-and-why-are-batches-used-in-machine-learning-acda4eb00763},
	year = {2019},
}
@report{Mais2002,
	author = {Jason Mais},
	city = {San Diego},
	institution = {SKF USA Inc.},
	month = {5},
	pages = {1-32},
	title = {Spectrum Analysis},
	year = {2002},
}

@web_page{Walleser2021,
	author = {Emil Walleser},
	journal = {Towards Data Science},
	month = {6},
	title = {Artificial Intelligence Has an Enormous Carbon Footprint},
	url = {https://towardsdatascience.com/artificial-intelligence-has-an-enormous-carbon-footprint-239290ebffe},
	year = {2021},
}
@web_page{Culurciello2018,
	author = {Eugenio Culurciello},
	journal = {Towards Data Science},
	month = {4},
	title = {The fall of RNN / LSTM},
	url = {https://towardsdatascience.com/the-fall-of-rnn-lstm-2d1594c74ce0},
	year = {2018},
}
@article{Bai2018,
	abstract = {For most deep learning practitioners, sequence modeling is synonymous with recurrent networks. Yet recent results indicate that convolutional architectures can outperform recurrent networks on tasks such as audio synthesis and machine translation. Given a new sequence modeling task or dataset, which architecture should one use? We conduct a systematic evaluation of generic convolutional and recurrent architectures for sequence modeling. The models are evaluated across a broad range of standard tasks that are commonly used to benchmark recurrent networks. Our results indicate that a simple convolutional architecture outperforms canonical recurrent networks such as LSTMs across a diverse range of tasks and datasets, while demonstrating longer effective memory. We conclude that the common association between sequence modeling and recurrent networks should be reconsidered, and convolutional networks should be regarded as a natural starting point for sequence modeling tasks. To assist related work, we have made code available at http://github.com/locuslab/TCN .},
	author = {Shaojie Bai and J. Zico Kolter and Vladlen Koltun},
	month = {3},
	volume    = {abs/2004.00433},
	journal   = {Computing Research Repository},
	volume    = {abs/1803.01271},
	title = {An Empirical Evaluation of Generic Convolutional and Recurrent Networks for Sequence Modeling},
	year = {2018},
}
@inproceedings{Wang2017,
	author = {Zhiguang Wang and Weizhong Yan and Tim Oates},
	city = {Anchorage, AK, USA},
	doi = {10.1109/IJCNN.2017.7966039},
	isbn = {978-1-5090-6182-2},
	booktitle={2017 International Joint Conference on Neural Networks (IJCNN)}, 
	journal = {2017 International Joint Conference on Neural Networks (IJCNN)},
	month = {5},
	publisher = {IEEE},
	title = {Time series classification from scratch with deep neural networks: A strong baseline},
	location = {Anchorage, AK, USA},
	year = {2017},
}
@book{pena2011,
	title={A course in time series analysis},
	author={Peña, Daniel and Tiao, George C and Tsay, Ruey S},
	volume={322},
	year={2011},
	publisher={John Wiley \& Sons}
}
@web_page{Singh2018,
	author = {Aishwarya Singh},
	journal = {Analytics Vidhya},
	month = {9},
	title = {A Multivariate Time Series Guide to Forecasting and Modeling},
	url = {https://www.analyticsvidhya.com/blog/2018/09/multivariate-time-series-guide-forecasting-modeling-python-codes/},
	year = {2018},
}
@incollection{Olkin2001,
	title = {Multivariate Analysis: Overview},
	editor = {Smelser, Neil J. and Baltes, Paul B.},
	booktitle = {International Encyclopedia of the Social and Behavioral Sciences},
	publisher = {Pergamon},
	address = {Oxford},
	pages = {10240-10247},
	year = {2001},
	isbn = {978-0-08-043076-8},
	doi = {https://doi.org/10.1016/B0-08-043076-7/00472-1},
	author = {Olkin, I. and Sampson, A.R.},
	abstract = {Multivariate analysis is the statistical study of data where multiple measurements are made on each experimental unit and where the relationships among multivariate measurements and their structure are important. A modern, overlapping categorization of multivariate analysis is: normal and general multivariate models and distribution theory; the study and measurement of relationships; probability computations of multidimensional regions; and the exploration of data structures and patterns. The multivariate normal distribution plays a central role in multivariate analysis in the same way that the univariate normal distribution plays a central role. The Student'st generalizes to Hotelling's T2 and the chi-square distribution to the Wishart distribution. Newer multivariate distributions can model data when the multivariate normal distribution is not adequate. For multidimensional data, relationships among the variables are fundamental to explore. Useful techniques to understand and quantify these include multivariate regression analysis and various correlational notions such as partial correlations and canonical correlations. Approaches to compute complicated multidimensional probabilities including obtaining lower bounds for the probabilities and using numerical approximation techniques. The exploration of structure and patterns for complex multivariate data sets is crucial for modern data analysis and data mining. Multivariate tools, useful in this context, include principal components analysis, canonical analysis, factor analysis, path analysis, structural equation methods, clustering, and discriminant analysis.}
}
@web_page{Fumo2017,
	author = {David Fumo},
	journal = {Towards Data Science},
	month = {4},
	title = {A Gentle Introduction To Neural Networks Series},
	url = {https://towardsdatascience.com/a-gentle-introduction-to-neural-networks-series-part-1-2b90b87795bc},
	year = {2017},
}
@web_page{Gad2018,
	author = {Ahmed Gad},
	journal = {Towards Data Science},
	month = {6},
	title = {Beginners Ask “How Many Hidden Layers/Neurons to Use in Artificial Neural Networks?”},
	url = {https://towardsdatascience.com/beginners-ask-how-many-hidden-layers-neurons-to-use-in-artificial-neural-networks-51466afa0d3e},
	year = {2018},
}
@web_page{Doshi2019,
	author = {Sanket Doshi},
	journal = {Towards Data Science},
	month = {1},
	title = {Various Optimization Algorithms For Training Neural Network},
	url = {https://towardsdatascience.com/optimizers-for-training-neural-network-59450d71caf6},
	year = {2019},
}
@book{Russell2010,
	title={Artificial Intelligence: A Modern Approach, 3rd Edition},
	author={Russell, Stuart and Norvig, Peter},
	volume={3},
	year={2010},
	publisher={Pearson}
}
@book{Hinton1999,
	title={Unsupervised Learning: Foundations of Neural Computation},
	author={Hinton, Geoffrey and Sejnowski, Terrence J. },
	volume={3},
	doi = {https://doi.org/10.7551/mitpress/7011.001.0001},
	year={1999},
	publisher={The MIT Press}
}
@web_page{Shung2018,
	author = {Koo Ping Shung},
	journal = {Towards Data Science},
	month = {3},
	title = {Accuracy, Precision, Recall or F1?},
	url = {https://towardsdatascience.com/accuracy-precision-recall-or-f1-331fb37c5cb9},
	year = {2018},
}
@web_page{Brownlee2017,
	author = {Jason Brownlee},
	journal = {Machine Learning Mastery},
	month = {7},
	title = {What is the Difference Between Test and Validation Datasets?},
	url = {https://machinelearningmastery.com/difference-test-validation-datasets/},
	year = {2017},
}
@web_page{Brownlee2019,
	author = {Jason Brownlee},
	journal = {Machine Learning Mastery},
	month = {1},
	title = {How to Choose Loss Functions When Training Deep Learning Neural Networks},
	url = {https://machinelearningmastery.com/how-to-choose-loss-functions-when-training-deep-learning-neural-networks/},
	year = {2019},
}
@web_page{Stöttner2019,
	author = {Timo Stöttner},
	journal = {Towards Data Science},
	month = {5},
	title = {Why Data should be Normalized before Training a Neural Network},
	url = {https://towardsdatascience.com/why-data-should-be-normalized-before-training-a-neural-network-c626b7f66c7d},
	year = {2019},
}
@web_page{Brownlee2019.2,
	author = {Jason Brownlee},
	journal = {Machine Learning Mastery},
	month = {1},
	title = {A Gentle Introduction to the Rectified Linear Unit (ReLU)},
	url = {https://machinelearningmastery.com/rectified-linear-activation-function-for-deep-learning-neural-networks/},
	year = {2019},
}
    @article{Forman2010,
	author = {Forman, George and Scholz, Martin},
	title = {Apples-to-Apples in Cross-Validation Studies: Pitfalls in Classifier Performance Measurement},
	year = {2010},
	issue_date = {June 2010},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	volume = {12},
	number = {1},
	issn = {1931-0145},
	doi = {10.1145/1882471.1882479},
	abstract = {Cross-validation is a mainstay for measuring performance and progress in machine learning. There are subtle differences in how exactly to compute accuracy, F-measure and Area Under the ROC Curve (AUC) in cross-validation studies. However, these details are not discussed in the literature, and incompatible methods are used by various papers and software packages. This leads to inconsistency across the research literature. Anomalies in performance calculations for particular folds and situations go undiscovered when they are buried in aggregated results over many folds and datasets, without ever a person looking at the intermediate performance measurements. This research note clarifies and illustrates the differences, and it provides guidance for how best to measure classification performance under cross-validation. In particular, there are several divergent methods used for computing F-measure, which is often recommended as a performance measure under class imbalance, e.g., for text classification domains and in one-vs.-all reductions of datasets having many classes. We show by experiment that all but one of these computation methods leads to biased measurements, especially under high class imbalance. This paper is of particular interest to those designing machine learning software libraries and researchers focused on high class imbalance.},
	journal = {SIGKDD Explor. Newsl.},
	month = {nov},
	pages = {49–57},
	numpages = {9}
}
@web_page{Candanedo2015,
	author = {Luis Candanedo},
	journal = {UCI Machine Learning Repository},
	month = {2},
	title = {Appliances energy prediction Data Set},
	url = {https://archive.ics.uci.edu/ml/datasets/Appliances+energy+prediction},
	year = {2015},
}
@web_page{Tan2018,
	author = {Li Tan},
	journal = {EE Times},
	month = {4},
	title = {Multirate DSP, part 1: Upsampling and downsampling},
	url = {https://www.eetimes.com/multirate-dsp-part-1-upsampling-and-downsampling/},
	year = {2018},
}
@web_page{Swaminathan2018,
	author = {Saishruthi Swaminathan},
	journal = {Towards Data Science},
	month = {5},
	title = {Logistic Regression — Detailed Overview},
	url = {https://towardsdatascience.com/logistic-regression-detailed-overview-46c4da4303bc},
	year = {2018},
}
@web_page{Otexts2021,
	author = {Otexts},
	journal = {Forecasting: Principles and Practice},
	month = {12},
	title = {Some simple forecasting methods},
	url = {https://otexts.com/fpp2/simple-methods.html},
	year = {2021},
	note = {Retrieved: 14/12/2021}, 
}
@web_page{wiki2021,
	author = {Wikipedia},
	journal = {Wikipedia},
	month = {12},
	title = {Logistic regression},
	url = {https://en.wikipedia.org/wiki/Logistic_regression},
	year = {2021},
	note = {Retrieved: 18/12/2021}, 
}

